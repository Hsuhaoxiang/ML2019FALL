{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "import cv2\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # define: encoder\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Conv2d(16, 32, 3, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.01, inplace=True)\n",
    "        )\n",
    " \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(320, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(50, 10),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(x.shape[0], -1)\n",
    "        logits = self.classifier(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# detect is gpu available.\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    device =torch.device('cuda:0')\n",
    "else:\n",
    "    device =torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# load data and normalize to [-1, 1]\n",
    "trainX = np.load('./trainX.npy')\n",
    "print(trainX.shape)\n",
    "trainX = np.transpose(trainX, (0, 3, 1, 2))/ 255.\n",
    "trainX = torch.Tensor(trainX)\n",
    "\n",
    "\n",
    "# if use_gpu, send model / data to GPU.\n",
    "if use_gpu:\n",
    "    autoencoder.cuda()\n",
    "    trainX = trainX.cuda()\n",
    "\n",
    "# Dataloader: train shuffle = True\n",
    "train_dataloader = DataLoader(trainX, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(trainX, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    # BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD, BCE, KLD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "#     BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    \n",
    "    loss = nn.L1Loss(reduction='sum')\n",
    "#     MSE = F.mse_loss(recon_x, x, size_average=False)\n",
    "    l1_loss = loss(recon_x, x)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return l1_loss + KLD, KLD, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | 8992/9024] loss: 851.515502933\n",
      "  Training  | Loss:3498.0372 \n",
      "\n",
      "\n",
      "[Epoch 2 | 8992/9024] loss: 1525.10913086\n",
      "  Training  | Loss:3524.9271 \n",
      "\n",
      "\n",
      "[Epoch 3 | 8992/9024] loss: 984.275817878\n",
      "  Training  | Loss:3452.3011 \n",
      "\n",
      "\n",
      "[Epoch 4 | 8992/9024] loss: 1303.05590820\n",
      "  Training  | Loss:3501.6713 \n",
      "\n",
      "\n",
      "[Epoch 5 | 8992/9024] loss: 878.359069829\n",
      "  Training  | Loss:3490.7313 \n",
      "\n",
      "\n",
      "[Epoch 6 | 8992/9024] loss: 1051.80712891\n",
      "  Training  | Loss:3472.7910 \n",
      "\n",
      "\n",
      "[Epoch 7 | 8992/9024] loss: 996.740295415\n",
      "  Training  | Loss:3447.7288 \n",
      "\n",
      "\n",
      "[Epoch 8 | 8992/9024] loss: 830.584167480\n",
      "  Training  | Loss:3490.6267 \n",
      "\n",
      "\n",
      "[Epoch 9 | 8992/9024] loss: 1004.85620117\n",
      "  Training  | Loss:3463.8416 \n",
      "\n",
      "\n",
      "[Epoch 10 | 8992/9024] loss: 1095.71691895\n",
      "  Training  | Loss:3522.2915 \n",
      "\n",
      "\n",
      "[Epoch 11 | 8992/9024] loss: 1598.37158203\n",
      "  Training  | Loss:3483.8668 \n",
      "\n",
      "\n",
      "[Epoch 12 | 8992/9024] loss: 799.922607422\n",
      "  Training  | Loss:3474.4562 \n",
      "\n",
      "\n",
      "[Epoch 13 | 8992/9024] loss: 1011.86444092\n",
      "  Training  | Loss:3491.9109 \n",
      "\n",
      "\n",
      "[Epoch 14 | 8992/9024] loss: 1160.96630859\n",
      "  Training  | Loss:3488.8970 \n",
      "\n",
      "\n",
      "[Epoch 15 | 8992/9024] loss: 1063.08947754\n",
      "  Training  | Loss:3539.7358 \n",
      "\n",
      "\n",
      "[Epoch 16 | 8992/9024] loss: 1169.26733398\n",
      "  Training  | Loss:3486.9457 \n",
      "\n",
      "\n",
      "[Epoch 17 | 8992/9024] loss: 988.531311043\n",
      "  Training  | Loss:3478.2063 \n",
      "\n",
      "\n",
      "[Epoch 18 | 8992/9024] loss: 1183.99951172\n",
      "  Training  | Loss:3461.2437 \n",
      "\n",
      "\n",
      "[Epoch 19 | 8992/9024] loss: 982.126892099\n",
      "  Training  | Loss:3505.3721 \n",
      "\n",
      "\n",
      "[Epoch 20 | 8992/9024] loss: 1452.85522461\n",
      "  Training  | Loss:3485.9248 \n",
      "\n",
      "\n",
      "[Epoch 21 | 8992/9024] loss: 1079.77221680\n",
      "  Training  | Loss:3472.3965 \n",
      "\n",
      "\n",
      "[Epoch 22 | 8992/9024] loss: 985.527954107\n",
      "  Training  | Loss:3465.8131 \n",
      "\n",
      "\n",
      "[Epoch 23 | 8992/9024] loss: 1447.11108398\n",
      "  Training  | Loss:3438.5983 \n",
      "\n",
      "\n",
      "[Epoch 24 | 8992/9024] loss: 1022.65533447\n",
      "  Training  | Loss:3505.3831 \n",
      "\n",
      "\n",
      "[Epoch 25 | 8992/9024] loss: 1128.21423340\n",
      "  Training  | Loss:3460.4417 \n",
      "\n",
      "\n",
      "[Epoch 26 | 8992/9024] loss: 881.719848638\n",
      "  Training  | Loss:3459.1176 \n",
      "\n",
      "\n",
      "[Epoch 27 | 8992/9024] loss: 1048.78161621\n",
      "  Training  | Loss:3482.4659 \n",
      "\n",
      "\n",
      "[Epoch 28 | 8992/9024] loss: 1181.80065918\n",
      "  Training  | Loss:3488.0030 \n",
      "\n",
      "\n",
      "[Epoch 29 | 8992/9024] loss: 935.759216313\n",
      "  Training  | Loss:3424.7066 \n",
      "\n",
      "\n",
      "[Epoch 30 | 8992/9024] loss: 1120.61267090\n",
      "  Training  | Loss:3480.8880 \n",
      "\n",
      "\n",
      "[Epoch 31 | 8992/9024] loss: 1273.16259766\n",
      "  Training  | Loss:3513.7483 \n",
      "\n",
      "\n",
      "[Epoch 32 | 8992/9024] loss: 1280.72766113\n",
      "  Training  | Loss:3486.0623 \n",
      "\n",
      "\n",
      "[Epoch 33 | 8992/9024] loss: 1087.22924805\n",
      "  Training  | Loss:3466.8905 \n",
      "\n",
      "\n",
      "[Epoch 34 | 8992/9024] loss: 1229.82067871\n",
      "  Training  | Loss:3454.2904 \n",
      "\n",
      "\n",
      "[Epoch 35 | 8992/9024] loss: 952.308837897\n",
      "  Training  | Loss:3503.3351 \n",
      "\n",
      "\n",
      "[Epoch 36 | 8992/9024] loss: 1125.98901367\n",
      "  Training  | Loss:3457.3454 \n",
      "\n",
      "\n",
      "[Epoch 37 | 8992/9024] loss: 1026.11950684\n",
      "  Training  | Loss:3485.6443 \n",
      "\n",
      "\n",
      "[Epoch 38 | 8992/9024] loss: 1045.14245605\n",
      "  Training  | Loss:3486.5260 \n",
      "\n",
      "\n",
      "[Epoch 39 | 8992/9024] loss: 790.102966318\n",
      "  Training  | Loss:3485.5760 \n",
      "\n",
      "\n",
      "[Epoch 40 | 8992/9024] loss: 1119.00097656\n",
      "  Training  | Loss:3460.0745 \n",
      "\n",
      "\n",
      "[Epoch 41 | 8992/9024] loss: 896.042724611\n",
      "  Training  | Loss:3489.5202 \n",
      "\n",
      "\n",
      "[Epoch 42 | 8992/9024] loss: 1183.27734375\n",
      "  Training  | Loss:3476.1686 \n",
      "\n",
      "\n",
      "[Epoch 43 | 8992/9024] loss: 870.584350596\n",
      "  Training  | Loss:3455.2234 \n",
      "\n",
      "\n",
      "[Epoch 44 | 8992/9024] loss: 1276.38122559\n",
      "  Training  | Loss:3461.8687 \n",
      "\n",
      "\n",
      "[Epoch 45 | 8992/9024] loss: 944.784423839\n",
      "  Training  | Loss:3516.3724 \n",
      "\n",
      "\n",
      "[Epoch 46 | 8992/9024] loss: 1075.45861816\n",
      "  Training  | Loss:3512.8621 \n",
      "\n",
      "\n",
      "[Epoch 47 | 8992/9024] loss: 972.174682621\n",
      "  Training  | Loss:3468.6657 \n",
      "\n",
      "\n",
      "[Epoch 48 | 8992/9024] loss: 1266.91845703\n",
      "  Training  | Loss:3418.8031 \n",
      "\n",
      "\n",
      "[Epoch 49 | 8992/9024] loss: 975.991455089\n",
      "  Training  | Loss:3472.1523 \n",
      "\n",
      "\n",
      "[Epoch 50 | 8992/9024] loss: 914.455017097\n",
      "  Training  | Loss:3480.0761 \n",
      "\n",
      "\n",
      "[Epoch 51 | 8992/9024] loss: 886.063476563\n",
      "  Training  | Loss:3455.0087 \n",
      "\n",
      "\n",
      "[Epoch 52 | 8992/9024] loss: 1291.99987793\n",
      "  Training  | Loss:3494.1621 \n",
      "\n",
      "\n",
      "[Epoch 53 | 8992/9024] loss: 814.015686041\n",
      "  Training  | Loss:3478.0598 \n",
      "\n",
      "\n",
      "[Epoch 54 | 8992/9024] loss: 1388.85534668\n",
      "  Training  | Loss:3448.3789 \n",
      "\n",
      "\n",
      "[Epoch 55 | 8992/9024] loss: 1157.21374512\n",
      "  Training  | Loss:3503.4501 \n",
      "\n",
      "\n",
      "[Epoch 56 | 8992/9024] loss: 2092.53002930\n",
      "  Training  | Loss:3484.8970 \n",
      "\n",
      "\n",
      "[Epoch 57 | 8992/9024] loss: 1027.80480957\n",
      "  Training  | Loss:3476.5267 \n",
      "\n",
      "\n",
      "[Epoch 58 | 8992/9024] loss: 938.853210456\n",
      "  Training  | Loss:3456.1213 \n",
      "\n",
      "\n",
      "[Epoch 59 | 8992/9024] loss: 1140.16296387\n",
      "  Training  | Loss:3457.3235 \n",
      "\n",
      "\n",
      "[Epoch 60 | 8992/9024] loss: 1063.34729004\n",
      "  Training  | Loss:3469.2596 \n",
      "\n",
      "\n",
      "[Epoch 61 | 8992/9024] loss: 859.396789553\n",
      "  Training  | Loss:3436.7342 \n",
      "\n",
      "\n",
      "[Epoch 62 | 8992/9024] loss: 1464.54638672\n",
      "  Training  | Loss:3435.1935 \n",
      "\n",
      "\n",
      "[Epoch 63 | 8992/9024] loss: 992.644714366\n",
      "  Training  | Loss:3463.3851 \n",
      "\n",
      "\n",
      "[Epoch 64 | 8992/9024] loss: 957.177795411\n",
      "  Training  | Loss:3431.4486 \n",
      "\n",
      "\n",
      "[Epoch 65 | 8992/9024] loss: 1169.21850586\n",
      "  Training  | Loss:3527.7702 \n",
      "\n",
      "\n",
      "[Epoch 66 | 8992/9024] loss: 969.450927737\n",
      "  Training  | Loss:3445.7285 \n",
      "\n",
      "\n",
      "[Epoch 67 | 8992/9024] loss: 2011.92871094\n",
      "  Training  | Loss:3461.3975 \n",
      "\n",
      "\n",
      "[Epoch 68 | 8992/9024] loss: 1159.25842285\n",
      "  Training  | Loss:3435.6090 \n",
      "\n",
      "\n",
      "[Epoch 69 | 8992/9024] loss: 844.707092297\n",
      "  Training  | Loss:3518.8145 \n",
      "\n",
      "\n",
      "[Epoch 70 | 8992/9024] loss: 848.711914063\n",
      "  Training  | Loss:3444.1753 \n",
      "\n",
      "\n",
      "[Epoch 71 | 8992/9024] loss: 1271.15405273\n",
      "  Training  | Loss:3426.9915 \n",
      "\n",
      "\n",
      "[Epoch 72 | 8992/9024] loss: 996.026855470\n",
      "  Training  | Loss:3468.5996 \n",
      "\n",
      "\n",
      "[Epoch 73 | 8992/9024] loss: 1107.85839844\n",
      "  Training  | Loss:3474.8544 \n",
      "\n",
      "\n",
      "[Epoch 74 | 8992/9024] loss: 1037.06701660\n",
      "  Training  | Loss:3485.5406 \n",
      "\n",
      "\n",
      "[Epoch 75 | 8992/9024] loss: 951.203674323\n",
      "  Training  | Loss:3446.5017 \n",
      "\n",
      "\n",
      "[Epoch 76 | 8992/9024] loss: 1318.85693359\n",
      "  Training  | Loss:3468.7145 \n",
      "\n",
      "\n",
      "[Epoch 77 | 8992/9024] loss: 910.356262219\n",
      "  Training  | Loss:3487.8551 \n",
      "\n",
      "\n",
      "[Epoch 78 | 8992/9024] loss: 1005.99780273\n",
      "  Training  | Loss:3464.5718 \n",
      "\n",
      "\n",
      "[Epoch 79 | 8992/9024] loss: 1908.38452148\n",
      "  Training  | Loss:3485.6188 \n",
      "\n",
      "\n",
      "[Epoch 80 | 8992/9024] loss: 1090.46130371\n",
      "  Training  | Loss:3473.5557 \n",
      "\n",
      "\n",
      "[Epoch 81 | 8992/9024] loss: 1089.07910156\n",
      "  Training  | Loss:3415.1866 \n",
      "\n",
      "\n",
      "[Epoch 82 | 8992/9024] loss: 1262.14404297\n",
      "  Training  | Loss:3431.2419 \n",
      "\n",
      "\n",
      "[Epoch 83 | 8992/9024] loss: 814.923583985\n",
      "  Training  | Loss:3467.9198 \n",
      "\n",
      "\n",
      "[Epoch 84 | 8992/9024] loss: 1109.88244629\n",
      "  Training  | Loss:3405.6661 \n",
      "\n",
      "\n",
      "[Epoch 85 | 8992/9024] loss: 1909.87817383\n",
      "  Training  | Loss:3428.1820 \n",
      "\n",
      "\n",
      "[Epoch 86 | 8992/9024] loss: 1110.86437988\n",
      "  Training  | Loss:3454.0988 \n",
      "\n",
      "\n",
      "[Epoch 87 | 8992/9024] loss: 884.719543466\n",
      "  Training  | Loss:3489.2897 \n",
      "\n",
      "\n",
      "[Epoch 88 | 8992/9024] loss: 1031.96899414\n",
      "  Training  | Loss:3458.8732 \n",
      "\n",
      "\n",
      "[Epoch 89 | 8992/9024] loss: 1276.24096680\n",
      "  Training  | Loss:3424.4514 \n",
      "\n",
      "\n",
      "[Epoch 90 | 8992/9024] loss: 1619.52539062\n",
      "  Training  | Loss:3480.4352 \n",
      "\n",
      "\n",
      "[Epoch 91 | 8992/9024] loss: 1079.88244629\n",
      "  Training  | Loss:3477.4550 \n",
      "\n",
      "\n",
      "[Epoch 92 | 8992/9024] loss: 1662.19824219\n",
      "  Training  | Loss:3453.8403 \n",
      "\n",
      "\n",
      "[Epoch 93 | 8992/9024] loss: 1269.87805176\n",
      "  Training  | Loss:3446.2984 \n",
      "\n",
      "\n",
      "[Epoch 94 | 8992/9024] loss: 781.966796881\n",
      "  Training  | Loss:3442.3125 \n",
      "\n",
      "\n",
      "[Epoch 95 | 8992/9024] loss: 1021.71704102\n",
      "  Training  | Loss:3437.2875 \n",
      "\n",
      "\n",
      "[Epoch 96 | 8992/9024] loss: 989.970092778\n",
      "  Training  | Loss:3479.2659 \n",
      "\n",
      "\n",
      "[Epoch 97 | 8992/9024] loss: 1323.06140137\n",
      "  Training  | Loss:3438.5205 \n",
      "\n",
      "\n",
      "[Epoch 98 | 8992/9024] loss: 899.283264166\n",
      "  Training  | Loss:3443.6400 \n",
      "\n",
      "\n",
      "[Epoch 99 | 8992/9024] loss: 1013.26104736\n",
      "  Training  | Loss:3501.3032 \n",
      "\n",
      "\n",
      "[Epoch 100 | 8992/9024] loss: 911.886474613\n",
      "  Training  | Loss:3443.5220 \n",
      "\n",
      "\n",
      "[Epoch 101 | 8992/9024] loss: 1096.26928711\n",
      "  Training  | Loss:3468.2425 \n",
      "\n",
      "\n",
      "[Epoch 102 | 8992/9024] loss: 1033.90612793\n",
      "  Training  | Loss:3474.4094 \n",
      "\n",
      "\n",
      "[Epoch 103 | 8992/9024] loss: 1013.51184082\n",
      "  Training  | Loss:3450.5517 \n",
      "\n",
      "\n",
      "[Epoch 104 | 8992/9024] loss: 917.961791991\n",
      "  Training  | Loss:3441.0459 \n",
      "\n",
      "\n",
      "[Epoch 105 | 8992/9024] loss: 1421.63562012\n",
      "  Training  | Loss:3472.2762 \n",
      "\n",
      "\n",
      "[Epoch 106 | 8992/9024] loss: 1017.05419922\n",
      "  Training  | Loss:3469.2065 \n",
      "\n",
      "\n",
      "[Epoch 107 | 8992/9024] loss: 1635.37438965\n",
      "  Training  | Loss:3445.4437 \n",
      "\n",
      "\n",
      "[Epoch 108 | 8992/9024] loss: 1137.56677246\n",
      "  Training  | Loss:3442.4125 \n",
      "\n",
      "\n",
      "[Epoch 109 | 8992/9024] loss: 927.961486826\n",
      "  Training  | Loss:3454.0305 \n",
      "\n",
      "\n",
      "[Epoch 110 | 8992/9024] loss: 1893.25402832\n",
      "  Training  | Loss:3475.4180 \n",
      "\n",
      "\n",
      "[Epoch 111 | 8992/9024] loss: 1386.02343750\n",
      "  Training  | Loss:3476.9484 \n",
      "\n",
      "\n",
      "[Epoch 112 | 8992/9024] loss: 1003.05786133\n",
      "  Training  | Loss:3425.6640 \n",
      "\n",
      "\n",
      "[Epoch 113 | 8992/9024] loss: 1025.60437012\n",
      "  Training  | Loss:3438.4620 \n",
      "\n",
      "\n",
      "[Epoch 114 | 8992/9024] loss: 1214.69287109\n",
      "  Training  | Loss:3445.4033 \n",
      "\n",
      "\n",
      "[Epoch 115 | 8992/9024] loss: 1145.67407227\n",
      "  Training  | Loss:3468.1727 \n",
      "\n",
      "\n",
      "[Epoch 116 | 8992/9024] loss: 941.543212894\n",
      "  Training  | Loss:3406.9390 \n",
      "\n",
      "\n",
      "[Epoch 117 | 8992/9024] loss: 966.321105965\n",
      "  Training  | Loss:3470.8838 \n",
      "\n",
      "\n",
      "[Epoch 118 | 8992/9024] loss: 1179.43518066\n",
      "  Training  | Loss:3446.0755 \n",
      "\n",
      "\n",
      "[Epoch 119 | 8992/9024] loss: 1515.52062988\n",
      "  Training  | Loss:3429.4274 \n",
      "\n",
      "\n",
      "[Epoch 120 | 8992/9024] loss: 1221.68298340\n",
      "  Training  | Loss:3413.4268 \n",
      "\n",
      "\n",
      "[Epoch 121 | 8992/9024] loss: 1579.62524414\n",
      "  Training  | Loss:3445.1319 \n",
      "\n",
      "\n",
      "[Epoch 122 | 8992/9024] loss: 931.172241216\n",
      "  Training  | Loss:3417.8734 \n",
      "\n",
      "\n",
      "[Epoch 123 | 8992/9024] loss: 1253.51354980\n",
      "  Training  | Loss:3397.3548 \n",
      "\n",
      "\n",
      "[Epoch 124 | 8992/9024] loss: 1112.13659668\n",
      "  Training  | Loss:3482.6115 \n",
      "\n",
      "\n",
      "[Epoch 125 | 8992/9024] loss: 960.815856939\n",
      "  Training  | Loss:3439.4076 \n",
      "\n",
      "\n",
      "[Epoch 126 | 8992/9024] loss: 907.195373543\n",
      "  Training  | Loss:3502.7054 \n",
      "\n",
      "\n",
      "[Epoch 127 | 8992/9024] loss: 1032.19860840\n",
      "  Training  | Loss:3439.3098 \n",
      "\n",
      "\n",
      "[Epoch 128 | 8992/9024] loss: 926.804321295\n",
      "  Training  | Loss:3477.7814 \n",
      "\n",
      "\n",
      "[Epoch 129 | 8992/9024] loss: 1176.16589355\n",
      "  Training  | Loss:3465.9002 \n",
      "\n",
      "\n",
      "[Epoch 130 | 8992/9024] loss: 1217.07348633\n",
      "  Training  | Loss:3437.9977 \n",
      "\n",
      "\n",
      "[Epoch 131 | 8992/9024] loss: 1078.47290039\n",
      "  Training  | Loss:3445.0779 \n",
      "\n",
      "\n",
      "[Epoch 132 | 8992/9024] loss: 1393.31860352\n",
      "  Training  | Loss:3406.3039 \n",
      "\n",
      "\n",
      "[Epoch 133 | 8992/9024] loss: 1090.27368164\n",
      "  Training  | Loss:3442.7281 \n",
      "\n",
      "\n",
      "[Epoch 134 | 8992/9024] loss: 1502.17126465\n",
      "  Training  | Loss:3476.5516 \n",
      "\n",
      "\n",
      "[Epoch 135 | 8992/9024] loss: 1311.57177734\n",
      "  Training  | Loss:3429.2518 \n",
      "\n",
      "\n",
      "[Epoch 136 | 8992/9024] loss: 930.411926276\n",
      "  Training  | Loss:3433.1574 \n",
      "\n",
      "\n",
      "[Epoch 137 | 8992/9024] loss: 1139.18188477\n",
      "  Training  | Loss:3417.9832 \n",
      "\n",
      "\n",
      "[Epoch 138 | 8992/9024] loss: 902.818786626\n",
      "  Training  | Loss:3425.2718 \n",
      "\n",
      "\n",
      "[Epoch 139 | 8992/9024] loss: 962.390808110\n",
      "  Training  | Loss:3432.1996 \n",
      "\n",
      "\n",
      "[Epoch 140 | 8992/9024] loss: 958.342102058\n",
      "  Training  | Loss:3427.0573 \n",
      "\n",
      "\n",
      "[Epoch 141 | 8992/9024] loss: 927.706237790\n",
      "  Training  | Loss:3417.6392 \n",
      "\n",
      "\n",
      "[Epoch 142 | 8992/9024] loss: 977.264648440\n",
      "  Training  | Loss:3437.6388 \n",
      "\n",
      "\n",
      "[Epoch 143 | 8992/9024] loss: 1477.62487793\n",
      "  Training  | Loss:3421.3471 \n",
      "\n",
      "\n",
      "[Epoch 144 | 8992/9024] loss: 900.451782236\n",
      "  Training  | Loss:3422.8066 \n",
      "\n",
      "\n",
      "[Epoch 145 | 8992/9024] loss: 962.228942872\n",
      "  Training  | Loss:3435.0644 \n",
      "\n",
      "\n",
      "[Epoch 146 | 8992/9024] loss: 982.335083017\n",
      "  Training  | Loss:3396.3691 \n",
      "\n",
      "\n",
      "[Epoch 147 | 8992/9024] loss: 1108.38183594\n",
      "  Training  | Loss:3396.6526 \n",
      "\n",
      "\n",
      "[Epoch 148 | 8992/9024] loss: 1036.35144043\n",
      "  Training  | Loss:3445.6580 \n",
      "\n",
      "\n",
      "[Epoch 149 | 8992/9024] loss: 1164.57189941\n",
      "  Training  | Loss:3426.6061 \n",
      "\n",
      "\n",
      "[Epoch 150 | 8992/9024] loss: 971.834960945\n",
      "  Training  | Loss:3423.8015 \n",
      "\n",
      "\n",
      "[Epoch 151 | 8992/9024] loss: 1224.17578125\n",
      "  Training  | Loss:3398.9578 \n",
      "\n",
      "\n",
      "[Epoch 152 | 8992/9024] loss: 1177.99890137\n",
      "  Training  | Loss:3409.2864 \n",
      "\n",
      "\n",
      "[Epoch 153 | 8992/9024] loss: 845.708312993\n",
      "  Training  | Loss:3417.3704 \n",
      "\n",
      "\n",
      "[Epoch 154 | 8992/9024] loss: 1084.61499023\n",
      "  Training  | Loss:3431.1691 \n",
      "\n",
      "\n",
      "[Epoch 155 | 8992/9024] loss: 1348.08508301\n",
      "  Training  | Loss:3387.9391 \n",
      "\n",
      "\n",
      "[Epoch 156 | 8992/9024] loss: 1098.52648926\n",
      "  Training  | Loss:3438.4337 \n",
      "\n",
      "\n",
      "[Epoch 157 | 8992/9024] loss: 1280.43444824\n",
      "  Training  | Loss:3395.4370 \n",
      "\n",
      "\n",
      "[Epoch 158 | 8992/9024] loss: 896.788269048\n",
      "  Training  | Loss:3443.6609 \n",
      "\n",
      "\n",
      "[Epoch 159 | 8992/9024] loss: 1036.42309570\n",
      "  Training  | Loss:3461.1679 \n",
      "\n",
      "\n",
      "[Epoch 160 | 8992/9024] loss: 986.515380863\n",
      "  Training  | Loss:3405.5145 \n",
      "\n",
      "\n",
      "[Epoch 161 | 8992/9024] loss: 1148.62280273\n",
      "  Training  | Loss:3410.5406 \n",
      "\n",
      "\n",
      "[Epoch 162 | 8992/9024] loss: 1189.41735840\n",
      "  Training  | Loss:3419.5194 \n",
      "\n",
      "\n",
      "[Epoch 163 | 8992/9024] loss: 1178.98986816\n",
      "  Training  | Loss:3392.1302 \n",
      "\n",
      "\n",
      "[Epoch 164 | 8992/9024] loss: 871.684204108\n",
      "  Training  | Loss:3394.5947 \n",
      "\n",
      "\n",
      "[Epoch 165 | 8992/9024] loss: 1229.77392578\n",
      "  Training  | Loss:3456.2550 \n",
      "\n",
      "\n",
      "[Epoch 166 | 8992/9024] loss: 1476.88574219\n",
      "  Training  | Loss:3397.1668 \n",
      "\n",
      "\n",
      "[Epoch 167 | 8992/9024] loss: 1196.06933594\n",
      "  Training  | Loss:3476.2780 \n",
      "\n",
      "\n",
      "[Epoch 168 | 8992/9024] loss: 1017.98712158\n",
      "  Training  | Loss:3392.4380 \n",
      "\n",
      "\n",
      "[Epoch 169 | 8992/9024] loss: 930.137451179\n",
      "  Training  | Loss:3433.7294 \n",
      "\n",
      "\n",
      "[Epoch 170 | 8992/9024] loss: 974.949279790\n",
      "  Training  | Loss:3416.2257 \n",
      "\n",
      "\n",
      "[Epoch 171 | 8992/9024] loss: 1382.26879883\n",
      "  Training  | Loss:3407.2615 \n",
      "\n",
      "\n",
      "[Epoch 172 | 8992/9024] loss: 810.756530762\n",
      "  Training  | Loss:3397.3900 \n",
      "\n",
      "\n",
      "[Epoch 173 | 8992/9024] loss: 1264.84069824\n",
      "  Training  | Loss:3432.0962 \n",
      "\n",
      "\n",
      "[Epoch 174 | 8992/9024] loss: 1296.23132324\n",
      "  Training  | Loss:3401.3944 \n",
      "\n",
      "\n",
      "[Epoch 175 | 8992/9024] loss: 987.862854008\n",
      "  Training  | Loss:3407.5234 \n",
      "\n",
      "\n",
      "[Epoch 176 | 8992/9024] loss: 1194.73950195\n",
      "  Training  | Loss:3386.7849 \n",
      "\n",
      "\n",
      "[Epoch 177 | 8992/9024] loss: 871.691467292\n",
      "  Training  | Loss:3412.6447 \n",
      "\n",
      "\n",
      "[Epoch 178 | 8992/9024] loss: 891.097656258\n",
      "  Training  | Loss:3430.5790 \n",
      "\n",
      "\n",
      "[Epoch 179 | 8992/9024] loss: 1314.10607910\n",
      "  Training  | Loss:3389.4828 \n",
      "\n",
      "\n",
      "[Epoch 180 | 8992/9024] loss: 1187.62524414\n",
      "  Training  | Loss:3436.5373 \n",
      "\n",
      "\n",
      "[Epoch 181 | 8992/9024] loss: 936.153381358\n",
      "  Training  | Loss:3390.7542 \n",
      "\n",
      "\n",
      "[Epoch 182 | 8992/9024] loss: 947.120971686\n",
      "  Training  | Loss:3415.7006 \n",
      "\n",
      "\n",
      "[Epoch 183 | 8992/9024] loss: 882.546142580\n",
      "  Training  | Loss:3435.0722 \n",
      "\n",
      "\n",
      "[Epoch 184 | 8992/9024] loss: 1286.19238281\n",
      "  Training  | Loss:3423.6266 \n",
      "\n",
      "\n",
      "[Epoch 185 | 8992/9024] loss: 1037.53344727\n",
      "  Training  | Loss:3360.8062 \n",
      "\n",
      "\n",
      "[Epoch 186 | 8992/9024] loss: 911.906005867\n",
      "  Training  | Loss:3375.8200 \n",
      "\n",
      "\n",
      "[Epoch 187 | 8992/9024] loss: 1237.69689941\n",
      "  Training  | Loss:3421.5996 \n",
      "\n",
      "\n",
      "[Epoch 188 | 8992/9024] loss: 1082.53234863\n",
      "  Training  | Loss:3457.7452 \n",
      "\n",
      "\n",
      "[Epoch 189 | 8992/9024] loss: 1055.62951660\n",
      "  Training  | Loss:3436.3091 \n",
      "\n",
      "\n",
      "[Epoch 190 | 8992/9024] loss: 974.976074223\n",
      "  Training  | Loss:3456.7783 \n",
      "\n",
      "\n",
      "[Epoch 191 | 8992/9024] loss: 895.414367689\n",
      "  Training  | Loss:3425.2483 \n",
      "\n",
      "\n",
      "[Epoch 192 | 8992/9024] loss: 2245.15234375\n",
      "  Training  | Loss:3394.2711 \n",
      "\n",
      "\n",
      "[Epoch 193 | 8992/9024] loss: 1258.42468262\n",
      "  Training  | Loss:3390.4006 \n",
      "\n",
      "\n",
      "[Epoch 194 | 8992/9024] loss: 885.157592776\n",
      "  Training  | Loss:3432.9957 \n",
      "\n",
      "\n",
      "[Epoch 195 | 8992/9024] loss: 1168.98095703\n",
      "  Training  | Loss:3382.6346 \n",
      "\n",
      "\n",
      "[Epoch 196 | 8992/9024] loss: 1145.25500488\n",
      "  Training  | Loss:3420.4377 \n",
      "\n",
      "\n",
      "[Epoch 197 | 8992/9024] loss: 907.170166025\n",
      "  Training  | Loss:3380.5301 \n",
      "\n",
      "\n",
      "[Epoch 198 | 8992/9024] loss: 1189.77416992\n",
      "  Training  | Loss:3407.2533 \n",
      "\n",
      "\n",
      "[Epoch 199 | 8992/9024] loss: 1225.90429688\n",
      "  Training  | Loss:3369.7131 \n",
      "\n",
      "\n",
      "[Epoch 200 | 8992/9024] loss: 850.511840825\n",
      "  Training  | Loss:3417.0870 \n",
      "\n",
      "\n",
      "[Epoch 201 | 8992/9024] loss: 883.669738778\n",
      "  Training  | Loss:3389.0187 \n",
      "\n",
      "\n",
      "[Epoch 202 | 8992/9024] loss: 963.298583988\n",
      "  Training  | Loss:3408.7934 \n",
      "\n",
      "\n",
      "[Epoch 203 | 8992/9024] loss: 1194.14099121\n",
      "  Training  | Loss:3420.2561 \n",
      "\n",
      "\n",
      "[Epoch 204 | 8992/9024] loss: 1075.55651855\n",
      "  Training  | Loss:3403.7562 \n",
      "\n",
      "\n",
      "[Epoch 205 | 8992/9024] loss: 1076.39904785\n",
      "  Training  | Loss:3383.5602 \n",
      "\n",
      "\n",
      "[Epoch 206 | 8992/9024] loss: 961.766174328\n",
      "  Training  | Loss:3389.7379 \n",
      "\n",
      "\n",
      "[Epoch 207 | 8992/9024] loss: 980.325256354\n",
      "  Training  | Loss:3365.4036 \n",
      "\n",
      "\n",
      "[Epoch 208 | 8992/9024] loss: 1181.60998535\n",
      "  Training  | Loss:3403.8029 \n",
      "\n",
      "\n",
      "[Epoch 209 | 8992/9024] loss: 1081.54028320\n",
      "  Training  | Loss:3407.7713 \n",
      "\n",
      "\n",
      "[Epoch 210 | 8992/9024] loss: 1113.33361816\n",
      "  Training  | Loss:3422.3689 \n",
      "\n",
      "\n",
      "[Epoch 211 | 8992/9024] loss: 828.824462896\n",
      "  Training  | Loss:3409.8130 \n",
      "\n",
      "\n",
      "[Epoch 212 | 8992/9024] loss: 1153.11706543\n",
      "  Training  | Loss:3430.4002 \n",
      "\n",
      "\n",
      "[Epoch 213 | 8992/9024] loss: 987.576110843\n",
      "  Training  | Loss:3385.0510 \n",
      "\n",
      "\n",
      "[Epoch 214 | 8992/9024] loss: 790.621643078\n",
      "  Training  | Loss:3369.0926 \n",
      "\n",
      "\n",
      "[Epoch 215 | 8992/9024] loss: 823.386413571\n",
      "  Training  | Loss:3434.1495 \n",
      "\n",
      "\n",
      "[Epoch 216 | 8992/9024] loss: 972.650451668\n",
      "  Training  | Loss:3475.0379 \n",
      "\n",
      "\n",
      "[Epoch 217 | 8992/9024] loss: 1393.27990723\n",
      "  Training  | Loss:3366.8029 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 218 | 8992/9024] loss: 1062.74645996\n",
      "  Training  | Loss:3414.5719 \n",
      "\n",
      "\n",
      "[Epoch 219 | 8992/9024] loss: 891.661315920\n",
      "  Training  | Loss:3408.7714 \n",
      "\n",
      "\n",
      "[Epoch 220 | 8992/9024] loss: 1249.51025391\n",
      "  Training  | Loss:3397.8026 \n",
      "\n",
      "\n",
      "[Epoch 221 | 8992/9024] loss: 1052.55920410\n",
      "  Training  | Loss:3384.0579 \n",
      "\n",
      "\n",
      "[Epoch 222 | 8992/9024] loss: 1172.29772949\n",
      "  Training  | Loss:3370.3172 \n",
      "\n",
      "\n",
      "[Epoch 223 | 8992/9024] loss: 981.078186048\n",
      "  Training  | Loss:3387.9638 \n",
      "\n",
      "\n",
      "[Epoch 224 | 8992/9024] loss: 1259.07177734\n",
      "  Training  | Loss:3405.3046 \n",
      "\n",
      "\n",
      "[Epoch 225 | 8992/9024] loss: 1118.61486816\n",
      "  Training  | Loss:3381.8136 \n",
      "\n",
      "\n",
      "[Epoch 226 | 8992/9024] loss: 1513.37988281\n",
      "  Training  | Loss:3381.6001 \n",
      "\n",
      "\n",
      "[Epoch 227 | 8992/9024] loss: 1171.89416504\n",
      "  Training  | Loss:3396.4887 \n",
      "\n",
      "\n",
      "[Epoch 228 | 8992/9024] loss: 1263.78869629\n",
      "  Training  | Loss:3376.6015 \n",
      "\n",
      "\n",
      "[Epoch 229 | 8992/9024] loss: 1030.07690430\n",
      "  Training  | Loss:3379.1813 \n",
      "\n",
      "\n",
      "[Epoch 230 | 8992/9024] loss: 783.670654303\n",
      "  Training  | Loss:3358.0424 \n",
      "\n",
      "\n",
      "[Epoch 231 | 8992/9024] loss: 962.391967779\n",
      "  Training  | Loss:3357.1134 \n",
      "\n",
      "\n",
      "[Epoch 232 | 8992/9024] loss: 1316.57556152\n",
      "  Training  | Loss:3412.3870 \n",
      "\n",
      "\n",
      "[Epoch 233 | 8992/9024] loss: 1104.51318359\n",
      "  Training  | Loss:3399.7323 \n",
      "\n",
      "\n",
      "[Epoch 234 | 8992/9024] loss: 1126.62609863\n",
      "  Training  | Loss:3387.5003 \n",
      "\n",
      "\n",
      "[Epoch 235 | 8992/9024] loss: 1628.40332031\n",
      "  Training  | Loss:3368.6143 \n",
      "\n",
      "\n",
      "[Epoch 236 | 8992/9024] loss: 1213.88256836\n",
      "  Training  | Loss:3367.7852 \n",
      "\n",
      "\n",
      "[Epoch 237 | 8992/9024] loss: 1140.49255371\n",
      "  Training  | Loss:3366.7890 \n",
      "\n",
      "\n",
      "[Epoch 238 | 8992/9024] loss: 1012.15124512\n",
      "  Training  | Loss:3401.9504 \n",
      "\n",
      "\n",
      "[Epoch 239 | 8992/9024] loss: 1328.62536621\n",
      "  Training  | Loss:3377.4346 \n",
      "\n",
      "\n",
      "[Epoch 240 | 8992/9024] loss: 1170.76000977\n",
      "  Training  | Loss:3407.8964 \n",
      "\n",
      "\n",
      "[Epoch 241 | 8992/9024] loss: 1074.46899414\n",
      "  Training  | Loss:3390.8417 \n",
      "\n",
      "\n",
      "[Epoch 242 | 8992/9024] loss: 889.347229004\n",
      "  Training  | Loss:3406.9832 \n",
      "\n",
      "\n",
      "[Epoch 243 | 8992/9024] loss: 1025.71508789\n",
      "  Training  | Loss:3405.3441 \n",
      "\n",
      "\n",
      "[Epoch 244 | 8992/9024] loss: 1928.49890137\n",
      "  Training  | Loss:3377.1062 \n",
      "\n",
      "\n",
      "[Epoch 245 | 8992/9024] loss: 1289.29333496\n",
      "  Training  | Loss:3438.4988 \n",
      "\n",
      "\n",
      "[Epoch 246 | 8992/9024] loss: 895.529235842\n",
      "  Training  | Loss:3381.7625 \n",
      "\n",
      "\n",
      "[Epoch 247 | 8992/9024] loss: 1094.93994141\n",
      "  Training  | Loss:3378.4897 \n",
      "\n",
      "\n",
      "[Epoch 248 | 8992/9024] loss: 887.783142092\n",
      "  Training  | Loss:3406.1655 \n",
      "\n",
      "\n",
      "[Epoch 249 | 8992/9024] loss: 1162.35412598\n",
      "  Training  | Loss:3369.4218 \n",
      "\n",
      "\n",
      "[Epoch 250 | 8992/9024] loss: 937.290771489\n",
      "  Training  | Loss:3349.6143 \n",
      "\n",
      "\n",
      "[Epoch 251 | 8992/9024] loss: 1166.98229980\n",
      "  Training  | Loss:3388.4412 \n",
      "\n",
      "\n",
      "[Epoch 252 | 8992/9024] loss: 1019.88104248\n",
      "  Training  | Loss:3375.2074 \n",
      "\n",
      "\n",
      "[Epoch 253 | 8992/9024] loss: 1153.06823730\n",
      "  Training  | Loss:3396.2649 \n",
      "\n",
      "\n",
      "[Epoch 254 | 8992/9024] loss: 1023.09527588\n",
      "  Training  | Loss:3402.7161 \n",
      "\n",
      "\n",
      "[Epoch 255 | 8992/9024] loss: 1083.26232910\n",
      "  Training  | Loss:3371.4148 \n",
      "\n",
      "\n",
      "[Epoch 256 | 8992/9024] loss: 1106.54895020\n",
      "  Training  | Loss:3384.8108 \n",
      "\n",
      "\n",
      "[Epoch 257 | 8992/9024] loss: 672.991088871\n",
      "  Training  | Loss:3415.3910 \n",
      "\n",
      "\n",
      "[Epoch 258 | 8992/9024] loss: 878.142395028\n",
      "  Training  | Loss:3372.1905 \n",
      "\n",
      "\n",
      "[Epoch 259 | 8992/9024] loss: 901.847900392\n",
      "  Training  | Loss:3395.1645 \n",
      "\n",
      "\n",
      "[Epoch 260 | 8992/9024] loss: 923.988769538\n",
      "  Training  | Loss:3384.0847 \n",
      "\n",
      "\n",
      "[Epoch 261 | 8992/9024] loss: 1409.69238281\n",
      "  Training  | Loss:3377.1481 \n",
      "\n",
      "\n",
      "[Epoch 262 | 8992/9024] loss: 1432.63183594\n",
      "  Training  | Loss:3356.3968 \n",
      "\n",
      "\n",
      "[Epoch 263 | 8992/9024] loss: 950.866760257\n",
      "  Training  | Loss:3348.0949 \n",
      "\n",
      "\n",
      "[Epoch 264 | 8992/9024] loss: 916.489318852\n",
      "  Training  | Loss:3370.4578 \n",
      "\n",
      "\n",
      "[Epoch 265 | 8992/9024] loss: 988.416992196\n",
      "  Training  | Loss:3385.6033 \n",
      "\n",
      "\n",
      "[Epoch 266 | 8992/9024] loss: 922.712524416\n",
      "  Training  | Loss:3396.8987 \n",
      "\n",
      "\n",
      "[Epoch 267 | 8992/9024] loss: 937.639343261\n",
      "  Training  | Loss:3375.5852 \n",
      "\n",
      "\n",
      "[Epoch 268 | 8992/9024] loss: 1068.27331543\n",
      "  Training  | Loss:3384.4400 \n",
      "\n",
      "\n",
      "[Epoch 269 | 8992/9024] loss: 1352.76428223\n",
      "  Training  | Loss:3412.7508 \n",
      "\n",
      "\n",
      "[Epoch 270 | 8992/9024] loss: 939.529235843\n",
      "  Training  | Loss:3409.7415 \n",
      "\n",
      "\n",
      "[Epoch 271 | 8992/9024] loss: 1108.42712402\n",
      "  Training  | Loss:3417.9461 \n",
      "\n",
      "\n",
      "[Epoch 272 | 8992/9024] loss: 855.028137213\n",
      "  Training  | Loss:3352.3373 \n",
      "\n",
      "\n",
      "[Epoch 273 | 8992/9024] loss: 1210.76196289\n",
      "  Training  | Loss:3358.3508 \n",
      "\n",
      "\n",
      "[Epoch 274 | 8992/9024] loss: 935.923889160\n",
      "  Training  | Loss:3377.9481 \n",
      "\n",
      "\n",
      "[Epoch 275 | 8992/9024] loss: 1217.58068848\n",
      "  Training  | Loss:3405.4401 \n",
      "\n",
      "\n",
      "[Epoch 276 | 8992/9024] loss: 1419.61096191\n",
      "  Training  | Loss:3397.7933 \n",
      "\n",
      "\n",
      "[Epoch 277 | 8992/9024] loss: 832.160461434\n",
      "  Training  | Loss:3395.7193 \n",
      "\n",
      "\n",
      "[Epoch 278 | 8992/9024] loss: 866.522705082\n",
      "  Training  | Loss:3377.6964 \n",
      "\n",
      "\n",
      "[Epoch 279 | 8992/9024] loss: 1199.09521484\n",
      "  Training  | Loss:3364.0879 \n",
      "\n",
      "\n",
      "[Epoch 280 | 8992/9024] loss: 1156.76940918\n",
      "  Training  | Loss:3351.3712 \n",
      "\n",
      "\n",
      "[Epoch 281 | 8992/9024] loss: 947.827331546\n",
      "  Training  | Loss:3348.3050 \n",
      "\n",
      "\n",
      "[Epoch 282 | 8992/9024] loss: 1095.49682617\n",
      "  Training  | Loss:3403.4905 \n",
      "\n",
      "\n",
      "[Epoch 283 | 8992/9024] loss: 1037.69592285\n",
      "  Training  | Loss:3385.8814 \n",
      "\n",
      "\n",
      "[Epoch 284 | 8992/9024] loss: 990.075927731\n",
      "  Training  | Loss:3373.1978 \n",
      "\n",
      "\n",
      "[Epoch 285 | 8992/9024] loss: 972.643310551\n",
      "  Training  | Loss:3360.4812 \n",
      "\n",
      "\n",
      "[Epoch 286 | 8992/9024] loss: 989.754882816\n",
      "  Training  | Loss:3407.0806 \n",
      "\n",
      "\n",
      "[Epoch 287 | 8992/9024] loss: 1010.57495117\n",
      "  Training  | Loss:3363.3933 \n",
      "\n",
      "\n",
      "[Epoch 288 | 8992/9024] loss: 901.681457528\n",
      "  Training  | Loss:3350.5238 \n",
      "\n",
      "\n",
      "[Epoch 289 | 8992/9024] loss: 1374.82788086\n",
      "  Training  | Loss:3377.1378 \n",
      "\n",
      "\n",
      "[Epoch 290 | 8992/9024] loss: 1031.77514648\n",
      "  Training  | Loss:3392.1983 \n",
      "\n",
      "\n",
      "[Epoch 291 | 8992/9024] loss: 1650.90551758\n",
      "  Training  | Loss:3381.8209 \n",
      "\n",
      "\n",
      "[Epoch 292 | 8992/9024] loss: 987.388244639\n",
      "  Training  | Loss:3337.9383 \n",
      "\n",
      "\n",
      "[Epoch 293 | 8992/9024] loss: 1700.20275879\n",
      "  Training  | Loss:3365.9288 \n",
      "\n",
      "\n",
      "[Epoch 294 | 8992/9024] loss: 1017.46704102\n",
      "  Training  | Loss:3388.3230 \n",
      "\n",
      "\n",
      "[Epoch 295 | 8992/9024] loss: 999.516479490\n",
      "  Training  | Loss:3353.1207 \n",
      "\n",
      "\n",
      "[Epoch 296 | 8992/9024] loss: 1145.46240234\n",
      "  Training  | Loss:3356.9636 \n",
      "\n",
      "\n",
      "[Epoch 297 | 8992/9024] loss: 1097.17919922\n",
      "  Training  | Loss:3404.8939 \n",
      "\n",
      "\n",
      "[Epoch 298 | 8992/9024] loss: 1058.18823242\n",
      "  Training  | Loss:3401.5649 \n",
      "\n",
      "\n",
      "[Epoch 299 | 8992/9024] loss: 1177.43383789\n",
      "  Training  | Loss:3342.8193 \n",
      "\n",
      "\n",
      "[Epoch 300 | 8992/9024] loss: 1049.49108887\n",
      "  Training  | Loss:3365.9361 \n",
      "\n",
      "\n",
      "[Epoch 301 | 8992/9024] loss: 1078.21630859\n",
      "  Training  | Loss:3362.5058 \n",
      "\n",
      "\n",
      "[Epoch 302 | 8992/9024] loss: 1035.16101074\n",
      "  Training  | Loss:3348.5564 \n",
      "\n",
      "\n",
      "[Epoch 303 | 8992/9024] loss: 1036.87097168\n",
      "  Training  | Loss:3357.0742 \n",
      "\n",
      "\n",
      "[Epoch 304 | 8992/9024] loss: 991.878051761\n",
      "  Training  | Loss:3359.4902 \n",
      "\n",
      "\n",
      "[Epoch 305 | 8992/9024] loss: 866.968872070\n",
      "  Training  | Loss:3363.6445 \n",
      "\n",
      "\n",
      "[Epoch 306 | 8992/9024] loss: 808.376770020\n",
      "  Training  | Loss:3400.8738 \n",
      "\n",
      "\n",
      "[Epoch 307 | 8992/9024] loss: 908.038452152\n",
      "  Training  | Loss:3343.8344 \n",
      "\n",
      "\n",
      "[Epoch 308 | 8992/9024] loss: 927.848388671\n",
      "  Training  | Loss:3344.8875 \n",
      "\n",
      "\n",
      "[Epoch 309 | 8992/9024] loss: 1067.50427246\n",
      "  Training  | Loss:3372.1133 \n",
      "\n",
      "\n",
      "[Epoch 310 | 8992/9024] loss: 1240.02233887\n",
      "  Training  | Loss:3379.4948 \n",
      "\n",
      "\n",
      "[Epoch 311 | 8992/9024] loss: 951.024597176\n",
      "  Training  | Loss:3389.1464 \n",
      "\n",
      "\n",
      "[Epoch 312 | 8992/9024] loss: 1013.77423096\n",
      "  Training  | Loss:3352.1700 \n",
      "\n",
      "\n",
      "[Epoch 313 | 8992/9024] loss: 990.149475103\n",
      "  Training  | Loss:3349.4818 \n",
      "\n",
      "\n",
      "[Epoch 314 | 8992/9024] loss: 864.470581059\n",
      "  Training  | Loss:3345.6322 \n",
      "\n",
      "\n",
      "[Epoch 315 | 8992/9024] loss: 1563.85839844\n",
      "  Training  | Loss:3364.7926 \n",
      "\n",
      "\n",
      "[Epoch 316 | 8992/9024] loss: 787.089599619\n",
      "  Training  | Loss:3326.6640 \n",
      "\n",
      "\n",
      "[Epoch 317 | 8992/9024] loss: 832.184875493\n",
      "  Training  | Loss:3374.7434 \n",
      "\n",
      "\n",
      "[Epoch 318 | 8992/9024] loss: 1116.61401367\n",
      "  Training  | Loss:3405.5755 \n",
      "\n",
      "\n",
      "[Epoch 319 | 8992/9024] loss: 1032.38867188\n",
      "  Training  | Loss:3366.8166 \n",
      "\n",
      "\n",
      "[Epoch 320 | 8992/9024] loss: 1265.60754395\n",
      "  Training  | Loss:3352.5332 \n",
      "\n",
      "\n",
      "[Epoch 321 | 8992/9024] loss: 1777.45910645\n",
      "  Training  | Loss:3351.7295 \n",
      "\n",
      "\n",
      "[Epoch 322 | 8992/9024] loss: 912.535217298\n",
      "  Training  | Loss:3360.0016 \n",
      "\n",
      "\n",
      "[Epoch 323 | 8992/9024] loss: 1058.44018555\n",
      "  Training  | Loss:3388.5299 \n",
      "\n",
      "\n",
      "[Epoch 324 | 8992/9024] loss: 1588.90307617\n",
      "  Training  | Loss:3353.4249 \n",
      "\n",
      "\n",
      "[Epoch 325 | 8992/9024] loss: 917.193603526\n",
      "  Training  | Loss:3353.6840 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 326 | 8992/9024] loss: 1043.45458984\n",
      "  Training  | Loss:3338.1970 \n",
      "\n",
      "\n",
      "[Epoch 327 | 8992/9024] loss: 976.125732422\n",
      "  Training  | Loss:3342.9570 \n",
      "\n",
      "\n",
      "[Epoch 328 | 8992/9024] loss: 809.192810062\n",
      "  Training  | Loss:3351.4441 \n",
      "\n",
      "\n",
      "[Epoch 329 | 8992/9024] loss: 963.396179203\n",
      "  Training  | Loss:3362.0759 \n",
      "\n",
      "\n",
      "[Epoch 330 | 8992/9024] loss: 1364.94067383\n",
      "  Training  | Loss:3351.3783 \n",
      "\n",
      "\n",
      "[Epoch 331 | 8992/9024] loss: 956.124511724\n",
      "  Training  | Loss:3382.6487 \n",
      "\n",
      "\n",
      "[Epoch 332 | 8992/9024] loss: 1171.33557129\n",
      "  Training  | Loss:3308.1791 \n",
      "\n",
      "\n",
      "[Epoch 333 | 8992/9024] loss: 878.784790049\n",
      "  Training  | Loss:3367.8387 \n",
      "\n",
      "\n",
      "[Epoch 334 | 8992/9024] loss: 1034.35498047\n",
      "  Training  | Loss:3341.6071 \n",
      "\n",
      "\n",
      "[Epoch 335 | 8992/9024] loss: 1256.91735840\n",
      "  Training  | Loss:3331.3833 \n",
      "\n",
      "\n",
      "[Epoch 336 | 8992/9024] loss: 960.797363285\n",
      "  Training  | Loss:3408.2332 \n",
      "\n",
      "\n",
      "[Epoch 337 | 8992/9024] loss: 844.273986825\n",
      "  Training  | Loss:3367.1678 \n",
      "\n",
      "\n",
      "[Epoch 338 | 8992/9024] loss: 930.779602050\n",
      "  Training  | Loss:3347.2371 \n",
      "\n",
      "\n",
      "[Epoch 339 | 8992/9024] loss: 1215.77832031\n",
      "  Training  | Loss:3340.5307 \n",
      "\n",
      "\n",
      "[Epoch 340 | 8992/9024] loss: 1050.10717773\n",
      "  Training  | Loss:3341.4299 \n",
      "\n",
      "\n",
      "[Epoch 341 | 8992/9024] loss: 1380.40930176\n",
      "  Training  | Loss:3351.3605 \n",
      "\n",
      "\n",
      "[Epoch 342 | 8992/9024] loss: 1449.58947754\n",
      "  Training  | Loss:3361.9418 \n",
      "\n",
      "\n",
      "[Epoch 343 | 8992/9024] loss: 1039.43737793\n",
      "  Training  | Loss:3358.0244 \n",
      "\n",
      "\n",
      "[Epoch 344 | 8992/9024] loss: 1069.23950195\n",
      "  Training  | Loss:3367.4627 \n",
      "\n",
      "\n",
      "[Epoch 345 | 8992/9024] loss: 938.243774415\n",
      "  Training  | Loss:3390.8824 \n",
      "\n",
      "\n",
      "[Epoch 346 | 8992/9024] loss: 849.811950686\n",
      "  Training  | Loss:3352.6179 \n",
      "\n",
      "\n",
      "[Epoch 347 | 8992/9024] loss: 953.713623050\n",
      "  Training  | Loss:3317.3139 \n",
      "\n",
      "\n",
      "[Epoch 348 | 8992/9024] loss: 1057.34582520\n",
      "  Training  | Loss:3378.5911 \n",
      "\n",
      "\n",
      "[Epoch 349 | 8992/9024] loss: 1058.14660645\n",
      "  Training  | Loss:3366.6935 \n",
      "\n",
      "\n",
      "[Epoch 350 | 8992/9024] loss: 1057.45007324\n",
      "  Training  | Loss:3338.4061 \n",
      "\n",
      "\n",
      "[Epoch 351 | 8992/9024] loss: 1100.94934082\n",
      "  Training  | Loss:3345.6602 \n",
      "\n",
      "\n",
      "[Epoch 352 | 8992/9024] loss: 1192.60083008\n",
      "  Training  | Loss:3351.9583 \n",
      "\n",
      "\n",
      "[Epoch 353 | 8992/9024] loss: 1082.27722168\n",
      "  Training  | Loss:3335.0992 \n",
      "\n",
      "\n",
      "[Epoch 354 | 8992/9024] loss: 1420.06823730\n",
      "  Training  | Loss:3345.2682 \n",
      "\n",
      "\n",
      "[Epoch 355 | 8992/9024] loss: 964.449646003\n",
      "  Training  | Loss:3356.5514 \n",
      "\n",
      "\n",
      "[Epoch 356 | 8992/9024] loss: 1115.37707520\n",
      "  Training  | Loss:3345.4513 \n",
      "\n",
      "\n",
      "[Epoch 357 | 8992/9024] loss: 1247.47753906\n",
      "  Training  | Loss:3323.5936 \n",
      "\n",
      "\n",
      "[Epoch 358 | 8992/9024] loss: 1132.01977539\n",
      "  Training  | Loss:3365.1190 \n",
      "\n",
      "\n",
      "[Epoch 359 | 8992/9024] loss: 1446.55761719\n",
      "  Training  | Loss:3333.7132 \n",
      "\n",
      "\n",
      "[Epoch 360 | 8992/9024] loss: 913.680603031\n",
      "  Training  | Loss:3337.1652 \n",
      "\n",
      "\n",
      "[Epoch 361 | 8992/9024] loss: 910.040466317\n",
      "  Training  | Loss:3358.8368 \n",
      "\n",
      "\n",
      "[Epoch 362 | 8992/9024] loss: 1266.50292969\n",
      "  Training  | Loss:3327.8882 \n",
      "\n",
      "\n",
      "[Epoch 363 | 8992/9024] loss: 959.627502448\n",
      "  Training  | Loss:3354.7981 \n",
      "\n",
      "\n",
      "[Epoch 364 | 8992/9024] loss: 940.318176271\n",
      "  Training  | Loss:3347.9951 \n",
      "\n",
      "\n",
      "[Epoch 365 | 8992/9024] loss: 1010.63293457\n",
      "  Training  | Loss:3343.2807 \n",
      "\n",
      "\n",
      "[Epoch 366 | 8992/9024] loss: 1114.07714844\n",
      "  Training  | Loss:3352.8390 \n",
      "\n",
      "\n",
      "[Epoch 367 | 8992/9024] loss: 1002.17456055\n",
      "  Training  | Loss:3367.6114 \n",
      "\n",
      "\n",
      "[Epoch 368 | 8992/9024] loss: 1187.61022949\n",
      "  Training  | Loss:3353.5585 \n",
      "\n",
      "\n",
      "[Epoch 369 | 8992/9024] loss: 957.674560557\n",
      "  Training  | Loss:3356.4587 \n",
      "\n",
      "\n",
      "[Epoch 370 | 8992/9024] loss: 941.240478527\n",
      "  Training  | Loss:3343.0289 \n",
      "\n",
      "\n",
      "[Epoch 371 | 8992/9024] loss: 1160.05456543\n",
      "  Training  | Loss:3342.7829 \n",
      "\n",
      "\n",
      "[Epoch 372 | 8992/9024] loss: 1015.19128418\n",
      "  Training  | Loss:3348.4022 \n",
      "\n",
      "\n",
      "[Epoch 373 | 8992/9024] loss: 1235.50952148\n",
      "  Training  | Loss:3322.6815 \n",
      "\n",
      "\n",
      "[Epoch 374 | 8992/9024] loss: 913.721313489\n",
      "  Training  | Loss:3365.1107 \n",
      "\n",
      "\n",
      "[Epoch 375 | 8992/9024] loss: 1179.97521973\n",
      "  Training  | Loss:3331.3491 \n",
      "\n",
      "\n",
      "[Epoch 376 | 8992/9024] loss: 1097.34936523\n",
      "  Training  | Loss:3336.3788 \n",
      "\n",
      "\n",
      "[Epoch 377 | 8992/9024] loss: 944.754760746\n",
      "  Training  | Loss:3340.9873 \n",
      "\n",
      "\n",
      "[Epoch 378 | 8992/9024] loss: 1082.49414062\n",
      "  Training  | Loss:3345.4358 \n",
      "\n",
      "\n",
      "[Epoch 379 | 8992/9024] loss: 817.588867197\n",
      "  Training  | Loss:3368.3057 \n",
      "\n",
      "\n",
      "[Epoch 380 | 8992/9024] loss: 1014.68994141\n",
      "  Training  | Loss:3338.2317 \n",
      "\n",
      "\n",
      "[Epoch 381 | 8992/9024] loss: 927.890930180\n",
      "  Training  | Loss:3379.6269 \n",
      "\n",
      "\n",
      "[Epoch 382 | 8992/9024] loss: 1148.34838867\n",
      "  Training  | Loss:3320.5969 \n",
      "\n",
      "\n",
      "[Epoch 383 | 8992/9024] loss: 893.306152343\n",
      "  Training  | Loss:3330.7111 \n",
      "\n",
      "\n",
      "[Epoch 384 | 8992/9024] loss: 1146.83874512\n",
      "  Training  | Loss:3329.7626 \n",
      "\n",
      "\n",
      "[Epoch 385 | 8992/9024] loss: 889.256225598\n",
      "  Training  | Loss:3354.4950 \n",
      "\n",
      "\n",
      "[Epoch 386 | 8992/9024] loss: 1159.37817383\n",
      "  Training  | Loss:3349.9947 \n",
      "\n",
      "\n",
      "[Epoch 387 | 8992/9024] loss: 932.993408206\n",
      "  Training  | Loss:3332.1951 \n",
      "\n",
      "\n",
      "[Epoch 388 | 8992/9024] loss: 911.641784678\n",
      "  Training  | Loss:3344.2323 \n",
      "\n",
      "\n",
      "[Epoch 389 | 8992/9024] loss: 998.527038574\n",
      "  Training  | Loss:3374.3009 \n",
      "\n",
      "\n",
      "[Epoch 390 | 8992/9024] loss: 1054.77868652\n",
      "  Training  | Loss:3307.2129 \n",
      "\n",
      "\n",
      "[Epoch 391 | 8992/9024] loss: 1154.47766113\n",
      "  Training  | Loss:3337.8949 \n",
      "\n",
      "\n",
      "[Epoch 392 | 8992/9024] loss: 1005.71136475\n",
      "  Training  | Loss:3367.7270 \n",
      "\n",
      "\n",
      "[Epoch 393 | 8992/9024] loss: 987.792907716\n",
      "  Training  | Loss:3382.6261 \n",
      "\n",
      "\n",
      "[Epoch 394 | 8992/9024] loss: 1175.72229004\n",
      "  Training  | Loss:3338.4552 \n",
      "\n",
      "\n",
      "[Epoch 395 | 8992/9024] loss: 947.718688967\n",
      "  Training  | Loss:3362.7788 \n",
      "\n",
      "\n",
      "[Epoch 396 | 8992/9024] loss: 814.770019536\n",
      "  Training  | Loss:3348.7933 \n",
      "\n",
      "\n",
      "[Epoch 397 | 8992/9024] loss: 823.338378913\n",
      "  Training  | Loss:3320.9074 \n",
      "\n",
      "\n",
      "[Epoch 398 | 8992/9024] loss: 1116.09020996\n",
      "  Training  | Loss:3319.4221 \n",
      "\n",
      "\n",
      "[Epoch 399 | 8992/9024] loss: 1350.13598633\n",
      "  Training  | Loss:3344.1380 \n",
      "\n",
      "\n",
      "[Epoch 400 | 8992/9024] loss: 878.880615235\n",
      "  Training  | Loss:3311.6982 \n",
      "\n",
      "\n",
      "[Epoch 401 | 8992/9024] loss: 1299.96057129\n",
      "  Training  | Loss:3349.2290 \n",
      "\n",
      "\n",
      "[Epoch 402 | 8992/9024] loss: 1197.52404785\n",
      "  Training  | Loss:3382.2383 \n",
      "\n",
      "\n",
      "[Epoch 403 | 8992/9024] loss: 911.784301768\n",
      "  Training  | Loss:3317.0465 \n",
      "\n",
      "\n",
      "[Epoch 404 | 8992/9024] loss: 965.429687504\n",
      "  Training  | Loss:3333.9546 \n",
      "\n",
      "\n",
      "[Epoch 405 | 8992/9024] loss: 1126.44873047\n",
      "  Training  | Loss:3288.3427 \n",
      "\n",
      "\n",
      "[Epoch 406 | 8992/9024] loss: 961.993164065\n",
      "  Training  | Loss:3324.7243 \n",
      "\n",
      "\n",
      "[Epoch 407 | 8992/9024] loss: 1142.01428223\n",
      "  Training  | Loss:3338.3107 \n",
      "\n",
      "\n",
      "[Epoch 408 | 8992/9024] loss: 936.887512211\n",
      "  Training  | Loss:3364.7088 \n",
      "\n",
      "\n",
      "[Epoch 409 | 8992/9024] loss: 1280.59411621\n",
      "  Training  | Loss:3377.9863 \n",
      "\n",
      "\n",
      "[Epoch 410 | 8992/9024] loss: 918.849792489\n",
      "  Training  | Loss:3298.7989 \n",
      "\n",
      "\n",
      "[Epoch 411 | 8992/9024] loss: 1240.09204102\n",
      "  Training  | Loss:3330.0189 \n",
      "\n",
      "\n",
      "[Epoch 412 | 8992/9024] loss: 988.809875495\n",
      "  Training  | Loss:3358.5391 \n",
      "\n",
      "\n",
      "[Epoch 413 | 8992/9024] loss: 954.426086438\n",
      "  Training  | Loss:3339.6085 \n",
      "\n",
      "\n",
      "[Epoch 414 | 8992/9024] loss: 904.528503429\n",
      "  Training  | Loss:3327.9957 \n",
      "\n",
      "\n",
      "[Epoch 415 | 8992/9024] loss: 1017.63842773\n",
      "  Training  | Loss:3341.6242 \n",
      "\n",
      "\n",
      "[Epoch 416 | 8992/9024] loss: 974.616699225\n",
      "  Training  | Loss:3319.1269 \n",
      "\n",
      "\n",
      "[Epoch 417 | 8992/9024] loss: 858.994934083\n",
      "  Training  | Loss:3328.3553 \n",
      "\n",
      "\n",
      "[Epoch 418 | 8992/9024] loss: 866.372253420\n",
      "  Training  | Loss:3339.7039 \n",
      "\n",
      "\n",
      "[Epoch 419 | 8992/9024] loss: 791.741394045\n",
      "  Training  | Loss:3349.2586 \n",
      "\n",
      "\n",
      "[Epoch 420 | 8992/9024] loss: 1062.12829590\n",
      "  Training  | Loss:3315.9262 \n",
      "\n",
      "\n",
      "[Epoch 421 | 8992/9024] loss: 914.883361829\n",
      "  Training  | Loss:3328.3327 \n",
      "\n",
      "\n",
      "[Epoch 422 | 8992/9024] loss: 994.673889165\n",
      "  Training  | Loss:3336.6057 \n",
      "\n",
      "\n",
      "[Epoch 423 | 8992/9024] loss: 1150.09216309\n",
      "  Training  | Loss:3343.9303 \n",
      "\n",
      "\n",
      "[Epoch 424 | 8992/9024] loss: 1275.78625488\n",
      "  Training  | Loss:3331.8712 \n",
      "\n",
      "\n",
      "[Epoch 425 | 8992/9024] loss: 894.346801763\n",
      "  Training  | Loss:3346.6148 \n",
      "\n",
      "\n",
      "[Epoch 426 | 8992/9024] loss: 977.035766607\n",
      "  Training  | Loss:3329.0350 \n",
      "\n",
      "\n",
      "[Epoch 427 | 8992/9024] loss: 889.385253912\n",
      "  Training  | Loss:3329.6623 \n",
      "\n",
      "\n",
      "[Epoch 428 | 8992/9024] loss: 1119.33239746\n",
      "  Training  | Loss:3313.3337 \n",
      "\n",
      "\n",
      "[Epoch 429 | 8992/9024] loss: 1068.21398926\n",
      "  Training  | Loss:3327.4480 \n",
      "\n",
      "\n",
      "[Epoch 430 | 8992/9024] loss: 956.005859382\n",
      "  Training  | Loss:3355.3601 \n",
      "\n",
      "\n",
      "[Epoch 431 | 8992/9024] loss: 1034.88281250\n",
      "  Training  | Loss:3342.8064 \n",
      "\n",
      "\n",
      "[Epoch 432 | 8992/9024] loss: 881.139770516\n",
      "  Training  | Loss:3342.0861 \n",
      "\n",
      "\n",
      "[Epoch 433 | 8992/9024] loss: 914.572448738\n",
      "  Training  | Loss:3349.7224 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 434 | 8992/9024] loss: 997.762146008\n",
      "  Training  | Loss:3278.8163 \n",
      "\n",
      "\n",
      "[Epoch 435 | 8992/9024] loss: 1067.60717773\n",
      "  Training  | Loss:3334.2494 \n",
      "\n",
      "\n",
      "[Epoch 436 | 8992/9024] loss: 756.799377446\n",
      "  Training  | Loss:3356.2752 \n",
      "\n",
      "\n",
      "[Epoch 437 | 8992/9024] loss: 1025.75610352\n",
      "  Training  | Loss:3307.2937 \n",
      "\n",
      "\n",
      "[Epoch 438 | 8992/9024] loss: 894.236145020\n",
      "  Training  | Loss:3321.5772 \n",
      "\n",
      "\n",
      "[Epoch 439 | 8992/9024] loss: 932.516662601\n",
      "  Training  | Loss:3347.2905 \n",
      "\n",
      "\n",
      "[Epoch 440 | 8992/9024] loss: 981.675537117\n",
      "  Training  | Loss:3321.0885 \n",
      "\n",
      "\n",
      "[Epoch 441 | 8992/9024] loss: 947.317260741\n",
      "  Training  | Loss:3373.1619 \n",
      "\n",
      "\n",
      "[Epoch 442 | 8992/9024] loss: 997.264038091\n",
      "  Training  | Loss:3318.1030 \n",
      "\n",
      "\n",
      "[Epoch 443 | 8992/9024] loss: 1288.18139648\n",
      "  Training  | Loss:3282.9663 \n",
      "\n",
      "\n",
      "[Epoch 444 | 8992/9024] loss: 1187.31335449\n",
      "  Training  | Loss:3367.9972 \n",
      "\n",
      "\n",
      "[Epoch 445 | 8992/9024] loss: 1234.10314941\n",
      "  Training  | Loss:3315.1944 \n",
      "\n",
      "\n",
      "[Epoch 446 | 8992/9024] loss: 1054.83142090\n",
      "  Training  | Loss:3336.2910 \n",
      "\n",
      "\n",
      "[Epoch 447 | 8992/9024] loss: 1061.53466797\n",
      "  Training  | Loss:3342.2071 \n",
      "\n",
      "\n",
      "[Epoch 448 | 8992/9024] loss: 1183.38781738\n",
      "  Training  | Loss:3310.0407 \n",
      "\n",
      "\n",
      "[Epoch 449 | 8992/9024] loss: 918.889526371\n",
      "  Training  | Loss:3292.9124 \n",
      "\n",
      "\n",
      "[Epoch 450 | 8992/9024] loss: 1074.96313477\n",
      "  Training  | Loss:3328.8954 \n",
      "\n",
      "\n",
      "[Epoch 451 | 8992/9024] loss: 960.991088871\n",
      "  Training  | Loss:3357.6197 \n",
      "\n",
      "\n",
      "[Epoch 452 | 8992/9024] loss: 1044.04016113\n",
      "  Training  | Loss:3311.3716 \n",
      "\n",
      "\n",
      "[Epoch 453 | 8992/9024] loss: 1036.33996582\n",
      "  Training  | Loss:3292.6689 \n",
      "\n",
      "\n",
      "[Epoch 454 | 8992/9024] loss: 1248.14111328\n",
      "  Training  | Loss:3307.0607 \n",
      "\n",
      "\n",
      "[Epoch 455 | 8992/9024] loss: 1014.49389648\n",
      "  Training  | Loss:3363.3489 \n",
      "\n",
      "\n",
      "[Epoch 456 | 8992/9024] loss: 837.684509288\n",
      "  Training  | Loss:3284.4703 \n",
      "\n",
      "\n",
      "[Epoch 457 | 8992/9024] loss: 885.496948243\n",
      "  Training  | Loss:3304.2453 \n",
      "\n",
      "\n",
      "[Epoch 458 | 8992/9024] loss: 958.981567381\n",
      "  Training  | Loss:3344.1557 \n",
      "\n",
      "\n",
      "[Epoch 459 | 8992/9024] loss: 1113.19812012\n",
      "  Training  | Loss:3321.2272 \n",
      "\n",
      "\n",
      "[Epoch 460 | 8992/9024] loss: 918.979431156\n",
      "  Training  | Loss:3356.6869 \n",
      "\n",
      "\n",
      "[Epoch 461 | 8992/9024] loss: 1058.27099609\n",
      "  Training  | Loss:3287.1793 \n",
      "\n",
      "\n",
      "[Epoch 462 | 8992/9024] loss: 1406.54870605\n",
      "  Training  | Loss:3342.3340 \n",
      "\n",
      "\n",
      "[Epoch 463 | 8992/9024] loss: 1181.28112793\n",
      "  Training  | Loss:3295.7800 \n",
      "\n",
      "\n",
      "[Epoch 464 | 8992/9024] loss: 1078.94946289\n",
      "  Training  | Loss:3301.1907 \n",
      "\n",
      "\n",
      "[Epoch 465 | 8992/9024] loss: 1347.03247070\n",
      "  Training  | Loss:3334.5926 \n",
      "\n",
      "\n",
      "[Epoch 466 | 8992/9024] loss: 956.408325203\n",
      "  Training  | Loss:3347.3636 \n",
      "\n",
      "\n",
      "[Epoch 467 | 8992/9024] loss: 1096.46008301\n",
      "  Training  | Loss:3337.3705 \n",
      "\n",
      "\n",
      "[Epoch 468 | 8992/9024] loss: 952.605407715\n",
      "  Training  | Loss:3310.7815 \n",
      "\n",
      "\n",
      "[Epoch 469 | 8992/9024] loss: 1974.10205078\n",
      "  Training  | Loss:3351.8835 \n",
      "\n",
      "\n",
      "[Epoch 470 | 8992/9024] loss: 940.149658205\n",
      "  Training  | Loss:3326.3090 \n",
      "\n",
      "\n",
      "[Epoch 471 | 8992/9024] loss: 1063.24511719\n",
      "  Training  | Loss:3309.6521 \n",
      "\n",
      "\n",
      "[Epoch 472 | 8992/9024] loss: 1171.03820801\n",
      "  Training  | Loss:3340.8542 \n",
      "\n",
      "\n",
      "[Epoch 473 | 8992/9024] loss: 875.684997569\n",
      "  Training  | Loss:3276.0034 \n",
      "\n",
      "\n",
      "[Epoch 474 | 8992/9024] loss: 787.694946291\n",
      "  Training  | Loss:3334.6694 \n",
      "\n",
      "\n",
      "[Epoch 475 | 8992/9024] loss: 1019.44970703\n",
      "  Training  | Loss:3331.2517 \n",
      "\n",
      "\n",
      "[Epoch 476 | 8992/9024] loss: 1131.38244629\n",
      "  Training  | Loss:3282.4865 \n",
      "\n",
      "\n",
      "[Epoch 477 | 8992/9024] loss: 1058.07214355\n",
      "  Training  | Loss:3322.2941 \n",
      "\n",
      "\n",
      "[Epoch 478 | 8992/9024] loss: 1153.82238770\n",
      "  Training  | Loss:3287.3541 \n",
      "\n",
      "\n",
      "[Epoch 479 | 8992/9024] loss: 1029.10266113\n",
      "  Training  | Loss:3315.1787 \n",
      "\n",
      "\n",
      "[Epoch 480 | 8992/9024] loss: 921.742370613\n",
      "  Training  | Loss:3362.2982 \n",
      "\n",
      "\n",
      "[Epoch 481 | 8992/9024] loss: 1547.69445801\n",
      "  Training  | Loss:3351.4286 \n",
      "\n",
      "\n",
      "[Epoch 482 | 8992/9024] loss: 1124.53198242\n",
      "  Training  | Loss:3318.9230 \n",
      "\n",
      "\n",
      "[Epoch 483 | 8992/9024] loss: 940.865783697\n",
      "  Training  | Loss:3349.8613 \n",
      "\n",
      "\n",
      "[Epoch 484 | 8992/9024] loss: 923.916137707\n",
      "  Training  | Loss:3291.6962 \n",
      "\n",
      "\n",
      "[Epoch 485 | 8992/9024] loss: 966.933410648\n",
      "  Training  | Loss:3301.6945 \n",
      "\n",
      "\n",
      "[Epoch 486 | 8992/9024] loss: 1087.45642090\n",
      "  Training  | Loss:3311.5980 \n",
      "\n",
      "\n",
      "[Epoch 487 | 8992/9024] loss: 1065.33666992\n",
      "  Training  | Loss:3311.0102 \n",
      "\n",
      "\n",
      "[Epoch 488 | 8992/9024] loss: 968.787048349\n",
      "  Training  | Loss:3317.3684 \n",
      "\n",
      "\n",
      "[Epoch 489 | 8992/9024] loss: 1099.86950684\n",
      "  Training  | Loss:3311.8325 \n",
      "\n",
      "\n",
      "[Epoch 490 | 8992/9024] loss: 777.290649412\n",
      "  Training  | Loss:3296.1009 \n",
      "\n",
      "\n",
      "[Epoch 491 | 8992/9024] loss: 1029.32580566\n",
      "  Training  | Loss:3304.9195 \n",
      "\n",
      "\n",
      "[Epoch 492 | 8992/9024] loss: 997.900146482\n",
      "  Training  | Loss:3302.8966 \n",
      "\n",
      "\n",
      "[Epoch 493 | 8992/9024] loss: 1224.27294922\n",
      "  Training  | Loss:3302.3274 \n",
      "\n",
      "\n",
      "[Epoch 494 | 8992/9024] loss: 1858.64440918\n",
      "  Training  | Loss:3317.8984 \n",
      "\n",
      "\n",
      "[Epoch 495 | 8992/9024] loss: 1117.49438477\n",
      "  Training  | Loss:3335.9555 \n",
      "\n",
      "\n",
      "[Epoch 496 | 8992/9024] loss: 906.982116703\n",
      "  Training  | Loss:3309.6816 \n",
      "\n",
      "\n",
      "[Epoch 497 | 8992/9024] loss: 845.525756844\n",
      "  Training  | Loss:3323.8950 \n",
      "\n",
      "\n",
      "[Epoch 498 | 8992/9024] loss: 964.925964367\n",
      "  Training  | Loss:3296.8171 \n",
      "\n",
      "\n",
      "[Epoch 499 | 8992/9024] loss: 1119.48559570\n",
      "  Training  | Loss:3293.1528 \n",
      "\n",
      "\n",
      "[Epoch 500 | 8992/9024] loss: 983.457763678\n",
      "  Training  | Loss:3280.3842 \n",
      "\n",
      "\n",
      "[Epoch 501 | 8992/9024] loss: 1207.89355469\n",
      "  Training  | Loss:3311.2256 \n",
      "\n",
      "\n",
      "[Epoch 502 | 8992/9024] loss: 698.044250492\n",
      "  Training  | Loss:3324.4717 \n",
      "\n",
      "\n",
      "[Epoch 503 | 8992/9024] loss: 1043.68383789\n",
      "  Training  | Loss:3291.4623 \n",
      "\n",
      "\n",
      "[Epoch 504 | 8992/9024] loss: 1076.10876465\n",
      "  Training  | Loss:3309.4684 \n",
      "\n",
      "\n",
      "[Epoch 505 | 8992/9024] loss: 1399.09130859\n",
      "  Training  | Loss:3329.3013 \n",
      "\n",
      "\n",
      "[Epoch 506 | 8992/9024] loss: 1826.84301758\n",
      "  Training  | Loss:3306.8079 \n",
      "\n",
      "\n",
      "[Epoch 507 | 8992/9024] loss: 946.997497566\n",
      "  Training  | Loss:3309.5486 \n",
      "\n",
      "\n",
      "[Epoch 508 | 8992/9024] loss: 1016.19647217\n",
      "  Training  | Loss:3286.9090 \n",
      "\n",
      "\n",
      "[Epoch 509 | 8992/9024] loss: 1189.30322266\n",
      "  Training  | Loss:3322.1514 \n",
      "\n",
      "\n",
      "[Epoch 510 | 8992/9024] loss: 903.116577155\n",
      "  Training  | Loss:3311.2124 \n",
      "\n",
      "\n",
      "[Epoch 511 | 8992/9024] loss: 862.532836912\n",
      "  Training  | Loss:3324.9418 \n",
      "\n",
      "\n",
      "[Epoch 512 | 8992/9024] loss: 1274.07446289\n",
      "  Training  | Loss:3320.0649 \n",
      "\n",
      "\n",
      "[Epoch 513 | 8992/9024] loss: 858.180541999\n",
      "  Training  | Loss:3292.4120 \n",
      "\n",
      "\n",
      "[Epoch 514 | 8992/9024] loss: 813.596618659\n",
      "  Training  | Loss:3297.1874 \n",
      "\n",
      "\n",
      "[Epoch 515 | 8992/9024] loss: 853.505493169\n",
      "  Training  | Loss:3326.0028 \n",
      "\n",
      "\n",
      "[Epoch 516 | 8992/9024] loss: 1011.43548584\n",
      "  Training  | Loss:3340.3387 \n",
      "\n",
      "\n",
      "[Epoch 517 | 8992/9024] loss: 1038.67895508\n",
      "  Training  | Loss:3341.9409 \n",
      "\n",
      "\n",
      "[Epoch 518 | 8992/9024] loss: 838.128662111\n",
      "  Training  | Loss:3311.3284 \n",
      "\n",
      "\n",
      "[Epoch 519 | 8992/9024] loss: 905.039978037\n",
      "  Training  | Loss:3338.6897 \n",
      "\n",
      "\n",
      "[Epoch 520 | 8992/9024] loss: 874.338195801\n",
      "  Training  | Loss:3325.4812 \n",
      "\n",
      "\n",
      "[Epoch 521 | 8992/9024] loss: 1316.67187500\n",
      "  Training  | Loss:3335.1239 \n",
      "\n",
      "\n",
      "[Epoch 522 | 8992/9024] loss: 813.717163091\n",
      "  Training  | Loss:3316.8787 \n",
      "\n",
      "\n",
      "[Epoch 523 | 8992/9024] loss: 1179.20983887\n",
      "  Training  | Loss:3298.6950 \n",
      "\n",
      "\n",
      "[Epoch 524 | 8992/9024] loss: 789.049926768\n",
      "  Training  | Loss:3332.4396 \n",
      "\n",
      "\n",
      "[Epoch 525 | 8992/9024] loss: 1419.95520020\n",
      "  Training  | Loss:3290.1821 \n",
      "\n",
      "\n",
      "[Epoch 526 | 8992/9024] loss: 793.458129880\n",
      "  Training  | Loss:3289.3697 \n",
      "\n",
      "\n",
      "[Epoch 527 | 8992/9024] loss: 1564.22619629\n",
      "  Training  | Loss:3299.4919 \n",
      "\n",
      "\n",
      "[Epoch 528 | 8992/9024] loss: 861.859313964\n",
      "  Training  | Loss:3304.4995 \n",
      "\n",
      "\n",
      "[Epoch 529 | 8992/9024] loss: 851.272094734\n",
      "  Training  | Loss:3310.7670 \n",
      "\n",
      "\n",
      "[Epoch 530 | 8992/9024] loss: 1108.36413574\n",
      "  Training  | Loss:3302.9918 \n",
      "\n",
      "\n",
      "[Epoch 531 | 8992/9024] loss: 845.368591317\n",
      "  Training  | Loss:3278.7408 \n",
      "\n",
      "\n",
      "[Epoch 532 | 8992/9024] loss: 1158.52258301\n",
      "  Training  | Loss:3318.6643 \n",
      "\n",
      "\n",
      "[Epoch 533 | 8992/9024] loss: 829.258728036\n",
      "  Training  | Loss:3307.9771 \n",
      "\n",
      "\n",
      "[Epoch 534 | 8992/9024] loss: 783.664611828\n",
      "  Training  | Loss:3306.5633 \n",
      "\n",
      "\n",
      "[Epoch 535 | 8992/9024] loss: 1325.85986328\n",
      "  Training  | Loss:3304.9360 \n",
      "\n",
      "\n",
      "[Epoch 536 | 8992/9024] loss: 1839.48291016\n",
      "  Training  | Loss:3320.1024 \n",
      "\n",
      "\n",
      "[Epoch 537 | 8992/9024] loss: 854.925048834\n",
      "  Training  | Loss:3314.2307 \n",
      "\n",
      "\n",
      "[Epoch 538 | 8992/9024] loss: 997.115295419\n",
      "  Training  | Loss:3301.1898 \n",
      "\n",
      "\n",
      "[Epoch 539 | 8992/9024] loss: 852.365722662\n",
      "  Training  | Loss:3275.1315 \n",
      "\n",
      "\n",
      "[Epoch 540 | 8992/9024] loss: 1203.71276855\n",
      "  Training  | Loss:3276.0848 \n",
      "\n",
      "\n",
      "[Epoch 541 | 8992/9024] loss: 1013.06762695\n",
      "  Training  | Loss:3299.3159 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 542 | 8992/9024] loss: 1144.27868652\n",
      "  Training  | Loss:3319.8751 \n",
      "\n",
      "\n",
      "[Epoch 543 | 8992/9024] loss: 1345.22619629\n",
      "  Training  | Loss:3325.8989 \n",
      "\n",
      "\n",
      "[Epoch 544 | 8992/9024] loss: 873.858642582\n",
      "  Training  | Loss:3293.4011 \n",
      "\n",
      "\n",
      "[Epoch 545 | 8992/9024] loss: 1115.42919922\n",
      "  Training  | Loss:3308.8303 \n",
      "\n",
      "\n",
      "[Epoch 546 | 8992/9024] loss: 1046.65185547\n",
      "  Training  | Loss:3283.3746 \n",
      "\n",
      "\n",
      "[Epoch 547 | 8992/9024] loss: 867.463806150\n",
      "  Training  | Loss:3286.0862 \n",
      "\n",
      "\n",
      "[Epoch 548 | 8992/9024] loss: 1174.99951172\n",
      "  Training  | Loss:3299.5344 \n",
      "\n",
      "\n",
      "[Epoch 549 | 8992/9024] loss: 787.450988773\n",
      "  Training  | Loss:3316.7732 \n",
      "\n",
      "\n",
      "[Epoch 550 | 8992/9024] loss: 838.168823241\n",
      "  Training  | Loss:3319.6071 \n",
      "\n",
      "\n",
      "[Epoch 551 | 8992/9024] loss: 967.579589849\n",
      "  Training  | Loss:3291.8284 \n",
      "\n",
      "\n",
      "[Epoch 552 | 8992/9024] loss: 836.193420412\n",
      "  Training  | Loss:3293.4102 \n",
      "\n",
      "\n",
      "[Epoch 553 | 8992/9024] loss: 882.919372563\n",
      "  Training  | Loss:3335.9207 \n",
      "\n",
      "\n",
      "[Epoch 554 | 8992/9024] loss: 1333.08862305\n",
      "  Training  | Loss:3299.2635 \n",
      "\n",
      "\n",
      "[Epoch 555 | 8992/9024] loss: 785.581542979\n",
      "  Training  | Loss:3295.8209 \n",
      "\n",
      "\n",
      "[Epoch 556 | 8992/9024] loss: 1359.94165039\n",
      "  Training  | Loss:3287.2397 \n",
      "\n",
      "\n",
      "[Epoch 557 | 8992/9024] loss: 2092.37792969\n",
      "  Training  | Loss:3314.5001 \n",
      "\n",
      "\n",
      "[Epoch 558 | 8992/9024] loss: 930.523742681\n",
      "  Training  | Loss:3309.8924 \n",
      "\n",
      "\n",
      "[Epoch 559 | 8992/9024] loss: 1095.84741211\n",
      "  Training  | Loss:3305.3603 \n",
      "\n",
      "\n",
      "[Epoch 560 | 8992/9024] loss: 839.215820312\n",
      "  Training  | Loss:3324.2420 \n",
      "\n",
      "\n",
      "[Epoch 561 | 8992/9024] loss: 1051.40673828\n",
      "  Training  | Loss:3300.6627 \n",
      "\n",
      "\n",
      "[Epoch 562 | 8992/9024] loss: 1067.59875488\n",
      "  Training  | Loss:3292.5903 \n",
      "\n",
      "\n",
      "[Epoch 563 | 8992/9024] loss: 1303.56713867\n",
      "  Training  | Loss:3263.0396 \n",
      "\n",
      "\n",
      "[Epoch 564 | 8992/9024] loss: 862.327758790\n",
      "  Training  | Loss:3288.5173 \n",
      "\n",
      "\n",
      "[Epoch 565 | 8992/9024] loss: 1200.63928223\n",
      "  Training  | Loss:3303.2317 \n",
      "\n",
      "\n",
      "[Epoch 566 | 8992/9024] loss: 968.466857917\n",
      "  Training  | Loss:3258.6679 \n",
      "\n",
      "\n",
      "[Epoch 567 | 5632/9024] loss: 3446.11840820\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f4fa6fb73c92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconsturct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We set criterion : L1 loss (or Mean Absolute Error, MAE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "# Now, we train 20 epochs.\n",
    "for epoch in range(1500):\n",
    "    model.train()\n",
    "    total_loss, best_loss = 0, 100\n",
    "    \"\"\"csie ta code\n",
    "    for x in train_dataloader:\n",
    "\n",
    "        latent, reconstruct = model(x)\n",
    "        loss = criterion(reconstruct, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cumulate_loss = loss.item() * x.shape[0]\n",
    "\n",
    "    print(f'Epoch { \"%03d\" % (epoch+1) }: Loss : { \"%.8f\" % (cumulate_loss / trainX.shape[0])}')\n",
    "    \"\"\"\n",
    "\n",
    "    for idx, image in enumerate(train_dataloader):\n",
    "        reconsturct , mean, var = model(image)\n",
    "        #print(reconsturct.shape,mean.shape,var.shape)\n",
    "        loss, bce, kld = loss_function(reconsturct, image, mean, var)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += (loss.item() / len(train_dataloader))\n",
    "        print('[Epoch %d | %d/%d] loss: %.8f' %((epoch+1), idx*32, len(train_dataloader)*32, loss.item()), end='\\r')\n",
    "    print(\"\\n  Training  | Loss:%.4f \\n\\n\" % total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_space finish\n",
      "(9000, 512)\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 9000 samples in 0.007s...\n",
      "[t-SNE] Computed neighbors for 9000 samples in 4.781s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 9000\n",
      "[t-SNE] Mean sigma: 1.285854\n",
      "[t-SNE] Computed conditional probabilities in 0.215s\n",
      "[t-SNE] Iteration 50: error = 91.6750641, gradient norm = 0.0000000 (50 iterations in 19.522s)\n",
      "[t-SNE] Iteration 50: gradient norm 0.000000. Finished.\n",
      "[t-SNE] KL divergence after 50 iterations with early exaggeration: 91.675064\n",
      "[t-SNE] Iteration 100: error = 4.5556688, gradient norm = 0.0030108 (50 iterations in 14.005s)\n",
      "[t-SNE] Iteration 150: error = 3.5694823, gradient norm = 0.0005077 (50 iterations in 18.428s)\n",
      "[t-SNE] Iteration 200: error = 3.4410646, gradient norm = 0.0002922 (50 iterations in 15.315s)\n",
      "[t-SNE] Iteration 250: error = 3.3654528, gradient norm = 0.0001917 (50 iterations in 16.629s)\n",
      "[t-SNE] Iteration 300: error = 3.3215101, gradient norm = 0.0001376 (50 iterations in 16.931s)\n",
      "[t-SNE] Iteration 350: error = 3.2960238, gradient norm = 0.0001032 (50 iterations in 15.863s)\n",
      "[t-SNE] Iteration 400: error = 3.2820172, gradient norm = 0.0000823 (50 iterations in 16.488s)\n",
      "[t-SNE] Iteration 450: error = 3.2746265, gradient norm = 0.0000682 (50 iterations in 17.612s)\n",
      "[t-SNE] Iteration 500: error = 3.2713025, gradient norm = 0.0000583 (50 iterations in 15.513s)\n",
      "[t-SNE] Iteration 550: error = 3.2702389, gradient norm = 0.0000510 (50 iterations in 15.081s)\n",
      "[t-SNE] Iteration 600: error = 3.2704217, gradient norm = 0.0000460 (50 iterations in 15.894s)\n",
      "[t-SNE] Iteration 650: error = 3.2712157, gradient norm = 0.0000420 (50 iterations in 19.559s)\n",
      "[t-SNE] Iteration 700: error = 3.2725105, gradient norm = 0.0000384 (50 iterations in 17.882s)\n",
      "[t-SNE] Iteration 750: error = 3.2737467, gradient norm = 0.0000357 (50 iterations in 15.711s)\n",
      "[t-SNE] Iteration 800: error = 3.2750289, gradient norm = 0.0000328 (50 iterations in 16.870s)\n",
      "[t-SNE] Iteration 850: error = 3.2762191, gradient norm = 0.0000305 (50 iterations in 18.815s)\n",
      "[t-SNE] Iteration 900: error = 3.2772532, gradient norm = 0.0000286 (50 iterations in 15.246s)\n",
      "[t-SNE] Iteration 900: did not make any progress during the last 300 episodes. Finished.\n",
      "[t-SNE] KL divergence after 900 iterations: 3.277253\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Collect the latents and stdardize it.\n",
    "latents = []\n",
    "latent_sapce = []\n",
    "for x in test_dataloader:\n",
    "    _,mu,var = model(x)\n",
    "    mu = mu.detach().cpu().numpy()\n",
    "    for i in range(mu.shape[0]):\n",
    "        latent_sapce.append(mu[i])\n",
    "        \n",
    "print('latent_space finish')\n",
    "latent_space = np.asarray(latent_sapce)\n",
    "\n",
    "print(latent_space.shape)\n",
    "latents = (latent_space - np.mean(latent_space, axis=0)) / np.std(latent_space, axis=0)\n",
    "\n",
    "# Use PCA to lower dim of latents and use K-means to clustering.\n",
    "pca = PCA(n_components=32, copy=False, whiten=True, svd_solver='full')\n",
    "latent_vec = pca.fit_transform(latents)\n",
    "latent_vec = TSNE(n_components = 3,verbose=5).fit_transform(latent_vec)\n",
    "#result = SpectralClustering(n_clusters=2,random_state = 2, n_init=10, gamma=1.0).fit_predict(latent_vec)\n",
    "result = KMeans(n_clusters=2, random_state=2, max_iter=1000).fit(latent_vec).labels_\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "latents = PCA(n_components=16).fit_transform(latents)\n",
    "result = KMeans(n_clusters = 2).fit(latents).labels_\n",
    "\"\"\"\n",
    "# We know first 5 labels are zeros, it's a mechanism to check are your answers\n",
    "# need to be flipped or not.\n",
    "print(np.sum(result[:5]))\n",
    "if np.sum(result[:5]) >= 3:\n",
    "    result = 1 - result\n",
    "\"\"\"\"\n",
    "if np.sum(result[:5]) != 0 or np.sum(result[:5])!=5:\n",
    "    print(\"redo\")\n",
    "\"\"\"\n",
    "# Generate your submission\n",
    "df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
    "df.to_csv('gogogo_11_19_14_30.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type VAE. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model,\"vae_11181612\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
