{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "import cv2\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import PIL.Image\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# detect is gpu available.\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"cuda\")\n",
    "    device =torch.device('cuda:0')\n",
    "else:\n",
    "    print(\"cpu\")\n",
    "    device =torch.device(\"cpu\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms =   transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "    \n",
    "class Image_data(Dataset):\n",
    "    def __init__(self,label,data,data_transforms):\n",
    "        self.data_path = data\n",
    "        self.label_path = label\n",
    "        self.data_transforms = data_transforms\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        trainX = np.load(self.data_path)\n",
    "        trainY = np.load(self.label_path)\n",
    "        image = PIL.Image.fromarray(np.uint8(trainX[index]))\n",
    "        trainY = torch.Tensor(trainY)\n",
    "        \n",
    "        return data_transforms(image),trainY[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(np.load(self.label_path))\n",
    "\n",
    "train_data = Image_data('./trainY.npy','./trainX.npy',data_transforms)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "validation_split = .1\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,sampler=train_sampler)\n",
    "val_loader = DataLoader(train_data, batch_size=batch_size,sampler=valid_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetEncoder(nn.Module):\n",
    "    \"\"\"LeNet encoder model for ADDA.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Init LeNet encoder.\"\"\"\n",
    "        super(LeNetEncoder, self).__init__()\n",
    "\n",
    "        self.restored = False\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(3, 32, kernel_size=5,stride = 2),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=5,stride = 2),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=5),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(2048, 512)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward the LeNet.\"\"\"\n",
    "        conv_out = self.encoder(input)\n",
    "        feat = self.fc1(conv_out.view(-1, 2048))\n",
    "        return feat\n",
    "\n",
    "\n",
    "class LeNetClassifier(nn.Module):\n",
    "    \"\"\"LeNet classifier model for ADDA.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Init LeNet encoder.\"\"\"\n",
    "        super(LeNetClassifier, self).__init__()\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, feat):\n",
    "        \"\"\"Forward the LeNet classifier.\"\"\"\n",
    "        out = F.dropout(F.relu(feat), training=self.training)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator model for source domain.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        \"\"\"Init discriminator.\"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.restored = False\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims, output_dims),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward the discriminator.\"\"\"\n",
    "        out = self.layer(input)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] Step [9/141]: loss=2.2763075828552246\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-cfb2a086b41b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# optimize source classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "encoder = LeNetEncoder()\n",
    "classifier = LeNetClassifier()\n",
    "\n",
    "\"\"\"Train classifier for source domain.\"\"\"\n",
    "####################\n",
    "# 1. setup network #\n",
    "####################\n",
    "\n",
    "# set train state for Dropout and BN layers\n",
    "encoder.train()\n",
    "classifier.train()\n",
    "\n",
    "# setup criterion and optimizer\n",
    "optimizer = optim.Adam(\n",
    "    list(encoder.parameters()) + list(classifier.parameters()),\n",
    "    lr = 1e-4,\n",
    "    betas=(0.5, 0.9))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "####################\n",
    "# 2. train network #\n",
    "####################\n",
    "\n",
    "for epoch in range(500):\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        # make images and labels variable\n",
    "        images = (images).to(device)\n",
    "        labels = (labels).to(device).long()\n",
    "\n",
    "        # zero gradients for optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute loss for critic\n",
    "        preds = classifier(encoder(images))\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        # optimize source classifier\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print step info\n",
    "\n",
    "        print(\"Epoch [{}/{}] Step [{}/{}]: loss={}\"\n",
    "              .format(epoch + 1,\n",
    "                      500,\n",
    "                      step + 1,\n",
    "                      len(train_loader),\n",
    "                      loss.data),end = '\\r')\n",
    "\n",
    "    # eval model on test set\n",
    "    eval_src(encoder, classifier, val_loader)\n",
    "\n",
    "\n",
    "\n",
    "def eval_src(encoder, classifier, loader):\n",
    "    \"\"\"Evaluate classifier for source domain.\"\"\"\n",
    "    # set eval state for Dropout and BN layers\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    # init loss and accuracy\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "\n",
    "    # set loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # evaluate network\n",
    "    for (images, labels) in loader:\n",
    "        images = (images).to(device)\n",
    "        labels = (labels).to(device).long()\n",
    "\n",
    "        preds = classifier(encoder(images))\n",
    "        loss += criterion(preds, labels).data\n",
    "\n",
    "        pred_cls = torch.max(preds, 1)[1]\n",
    "        for idx ,ans in enumerate(pred_cls):\n",
    "            if ans==labels[idx]:\n",
    "                acc += 1\n",
    "\n",
    "    loss /= len(loader)\n",
    "    acc /= (len(loader.dataset)/10)\n",
    "\n",
    "    print(\"Avg Loss = {}, Avg Accuracy = {:2%}\".format(loss, acc))\n",
    "    \n",
    "    \n",
    "train_src(encoder, classifier, train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LeNetEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LeNetClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(encoder, './feature_exactor.pt')\n",
    "torch.save(classifier, './classifier.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
