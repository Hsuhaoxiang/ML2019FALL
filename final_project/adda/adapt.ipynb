{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "import cv2\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import PIL.Image\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# detect is gpu available.\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"cuda\")\n",
    "    device =torch.device('cuda:0')\n",
    "else:\n",
    "    print(\"cpu\")\n",
    "    device =torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms =   transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "    \n",
    "class Image_data(Dataset):\n",
    "    def __init__(self,label,data,data_transforms):\n",
    "        self.data_path = data\n",
    "        self.label_path = label\n",
    "        self.data_transforms = data_transforms\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        trainX = np.load(self.data_path)\n",
    "        trainY = np.load(self.label_path)\n",
    "        image = PIL.Image.fromarray(np.uint8(trainX[index]))\n",
    "        trainY = torch.Tensor(trainY)\n",
    "        \n",
    "        return data_transforms(image),trainY[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(np.load(self.label_path))\n",
    "\n",
    "train_data = Image_data('./trainY.npy','./trainX.npy',data_transforms)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "validation_split = .1\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,sampler=train_sampler)\n",
    "val_loader = DataLoader(train_data, batch_size=batch_size,sampler=valid_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms =   transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ])\n",
    "class Test_data(Dataset):\n",
    "    def __init__(self,data,data_transforms):\n",
    "        self.data_path = data\n",
    "        self.data_transforms = data_transforms\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        trainX = np.load(self.data_path)\n",
    "        color_img = cv2.cvtColor(trainX[index],cv2.COLOR_GRAY2RGB)\n",
    "        image = PIL.Image.fromarray(np.uint8(color_img))\n",
    "\n",
    "        \n",
    "        return test_transforms(image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(np.load(self.data_path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_data = Test_data('testX.npy',test_transforms)\n",
    "test_loader = DataLoader(test_data , batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetEncoder(nn.Module):\n",
    "    \"\"\"LeNet encoder model for ADDA.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Init LeNet encoder.\"\"\"\n",
    "        super(LeNetEncoder, self).__init__()\n",
    "\n",
    "        self.restored = False\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(3, 32, kernel_size=5,stride = 2),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=5,stride = 2),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=5),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(2048, 512)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward the LeNet.\"\"\"\n",
    "        conv_out = self.encoder(input)\n",
    "        feat = self.fc1(conv_out.view(-1, 2048))\n",
    "        return feat\n",
    "\n",
    "\n",
    "class LeNetClassifier(nn.Module):\n",
    "    \"\"\"LeNet classifier model for ADDA.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Init LeNet encoder.\"\"\"\n",
    "        super(LeNetClassifier, self).__init__()\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, feat):\n",
    "        \"\"\"Forward the LeNet classifier.\"\"\"\n",
    "        out = F.dropout(F.relu(feat), training=self.training)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator model for source domain.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        \"\"\"Init discriminator.\"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.restored = False\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims, output_dims),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward the discriminator.\"\"\"\n",
    "        out = self.layer(input)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/500] Step [2/141]:d_loss=0.37395 g_loss=1.84836 acc=1.0000000\r"
     ]
    }
   ],
   "source": [
    "tgt_encoder = (torch.load(\"./feature_extractor.pt\"))\n",
    "src_encoder  = (torch.load(\"./feature_extractor.pt\"))\n",
    "critic = Discriminator(512,128,2)\n",
    "\"\"\"Train encoder for target domain \"\"\"\n",
    "####################\n",
    "# 1. setup network #\n",
    "####################\n",
    "\n",
    "# set train state for Dropout and BN layers\n",
    "tgt_encoder.train()\n",
    "critic.train()\n",
    "\n",
    "# setup criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_tgt = optim.Adam(tgt_encoder.parameters(),\n",
    "                           lr = 1e-4,\n",
    "                           betas=(0.5, 0.9))\n",
    "optimizer_critic = optim.Adam(critic.parameters(),\n",
    "                              lr = 1e-4,\n",
    "                               betas=(0.5, 0.9))\n",
    "len_data_loader = min(len(train_loader), len(test_loader))\n",
    "\n",
    "####################\n",
    "# 2. train network #\n",
    "####################\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    # zip source and target data pair\n",
    "    data_zip = enumerate(zip(train_loader, test_loader))\n",
    "    for step, ((images_src, _), (images_tgt)) in data_zip:\n",
    "        ###########################\n",
    "        # 2.1 train discriminator #\n",
    "        ###########################\n",
    "\n",
    "        # make images variable\n",
    "        images_src = (images_src).to(device)\n",
    "        images_tgt = (images_tgt).to(device)\n",
    "\n",
    "        # zero gradients for optimizer\n",
    "        optimizer_critic.zero_grad()\n",
    "\n",
    "        # extract and concat features\n",
    "        feat_src = src_encoder(images_src)\n",
    "        feat_tgt = tgt_encoder(images_tgt)\n",
    "        feat_concat = torch.cat((feat_src, feat_tgt), 0)\n",
    "\n",
    "        # predict on discriminator\n",
    "        pred_concat = critic(feat_concat.detach())\n",
    "        # prepare real and fake label\n",
    "        label_src = torch.ones(feat_src.size(0)).long().to(device)\n",
    "        label_tgt = torch.zeros(feat_tgt.size(0)).long().to(device)\n",
    "\n",
    "        label_concat = torch.cat((label_src, label_tgt))\n",
    "\n",
    "        \n",
    "        # compute loss for critic\n",
    "        print(pred_concat)\n",
    "        loss_critic = criterion(pred_concat, label_concat)\n",
    "        \n",
    "        loss_critic.backward()\n",
    "\n",
    "        # optimize critic\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        pred_cls = torch.squeeze(pred_concat.max(1)[1])\n",
    "        acc = (pred_cls == label_concat).float().mean()\n",
    "\n",
    "        ############################\n",
    "        # 2.2 train target encoder #\n",
    "        ############################\n",
    "\n",
    "        # zero gradients for optimizer\n",
    "        optimizer_critic.zero_grad()\n",
    "        optimizer_tgt.zero_grad()\n",
    "\n",
    "        # extract and target features\n",
    "        feat_tgt = tgt_encoder(images_tgt)\n",
    "\n",
    "        # predict on discriminator\n",
    "        pred_tgt = critic(feat_tgt)\n",
    "\n",
    "        # prepare fake labels\n",
    "        label_tgt = (torch.ones(feat_tgt.size(0)).long()).to(device)\n",
    "\n",
    "        # compute loss for target encoder\n",
    "        loss_tgt = criterion(pred_tgt, label_tgt)\n",
    "        loss_tgt.backward()\n",
    "\n",
    "        # optimize target encoder\n",
    "        optimizer_tgt.step()\n",
    "\n",
    "        #######################\n",
    "        # 2.3 print step info #\n",
    "        #######################\n",
    "        print(\"Epoch [{}/{}] Step [{}/{}]:\"\n",
    "              \"d_loss={:.5f} g_loss={:.5f} acc={:.5f}\"\n",
    "              .format(epoch + 1,\n",
    "                      epochs,\n",
    "                      step + 1,\n",
    "                      len_data_loader,\n",
    "                      loss_critic.data,\n",
    "                      loss_tgt.data,\n",
    "                      acc.data),end ='\\r')\n",
    "\n",
    "    #############################\n",
    "    # 2.4 save model parameters #\n",
    "    #############################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
