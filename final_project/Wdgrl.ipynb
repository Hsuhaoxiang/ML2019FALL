{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "import cv2\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "#from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Function\n",
    "\n",
    "#from tqdm.autonotebook import tqdm\n",
    "#from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # define: encoder\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Conv2d(16, 32, 3, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.01, inplace=True)\n",
    "        )\n",
    " \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(32, 10),\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(x.shape[0], -1)\n",
    "        logits = self.classifier(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect is gpu available.\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    device =torch.device('cuda:0')\n",
    "else:\n",
    "    device =torch.device(\"cpu\")\n",
    "class Image_data(Dataset):\n",
    "    def __init__(self,label,data):\n",
    "        self.data_path = data\n",
    "        self.label_path = label\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        trainX = np.load(self.data_path)\n",
    "        trainY = np.load(self.label_path)\n",
    "        trainX = np.transpose(trainX, (0, 3, 1, 2))\n",
    "        trainX = torch.Tensor(trainX)\n",
    "        trainY = torch.Tensor(trainY)\n",
    "        return trainX[index],trainY[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(np.load(self.label_path))\n",
    "\n",
    "train_data = Image_data('./trainY.npy','./trainX.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "#train_dataloader = DataLoader(train_data, batch_size=batch_size,sampler=train_sampler)\n",
    "#val_dataloader = DataLoader(train_data, batch_size=batch_size,sampler=valid_sampler)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_data,batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "test_data = np.load('testX.npy')\n",
    "\n",
    "\n",
    "rgb_test_data = []\n",
    "for image in test_data:\n",
    "    padding =np.zeros((image.shape[0]+4,image.shape[1]+4),dtype = np.uint8)\n",
    "    for i in range(image.shape[0]):\n",
    "        for j in range(image.shape[1]):\n",
    "            padding[i+1][j+1] = image[i][j]\n",
    "    color_img = cv2.cvtColor(padding,cv2.COLOR_GRAY2RGB)\n",
    "    rgb_test_data.append(color_img)\n",
    "rgb_test_data = np.array(rgb_test_data)\n",
    "test_data = np.transpose(rgb_test_data, (0, 3, 1, 2))\n",
    "test_data = torch.Tensor(test_data)\n",
    "\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer from:\n",
    "    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\n",
    "    Forward pass is the identity function. In the backward pass,\n",
    "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        lambda_ = ctx.lambda_\n",
    "        lambda_ = grads.new_tensor(lambda_)\n",
    "        dx = -lambda_ * grads\n",
    "        return dx, None\n",
    "\n",
    "\n",
    "class GradientReversal(torch.nn.Module):\n",
    "    def __init__(self, lambda_=1):\n",
    "        super(GradientReversal, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 363/500 [20:29<03:30,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 001: critic_loss=1.3480\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 363/500 [32:14<03:30,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 002: critic_loss=-0.3683\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 363/500 [44:26<03:30,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 003: critic_loss=-0.3036\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 363/500 [56:35<03:30,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 004: critic_loss=-0.3127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 363/500 [1:08:15<03:30,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 005: critic_loss=-0.2557\n"
     ]
    }
   ],
   "source": [
    "clf_model = Net()\n",
    "clf_model.load_state_dict(torch.load(\"./model.pt\"))\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from tqdm.notebook import  trange\n",
    "\n",
    "#import config\n",
    "#from data import MNISTM\n",
    "#from models import Net\n",
    "#from utils import loop_iterable, set_requires_grad, GrayscaleToRgb\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def set_requires_grad(model, requires_grad=True):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = requires_grad\n",
    "\n",
    "def loop_iterable(iterable):\n",
    "    while True:\n",
    "        yield from iterable\n",
    "\n",
    "def gradient_penalty(critic, h_s, h_t):\n",
    "    # based on: https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py#L116\n",
    "    alpha = torch.rand(h_s.size(0), 1).to(device)\n",
    "    differences = h_t - h_s\n",
    "    interpolates = h_s + (alpha * differences)\n",
    "    interpolates = torch.stack([interpolates, h_s, h_t]).requires_grad_()\n",
    "\n",
    "    preds = critic(interpolates)\n",
    "    gradients = grad(preds, interpolates,\n",
    "                     grad_outputs=torch.ones_like(preds),\n",
    "                     retain_graph=True, create_graph=True)[0]\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1)**2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "feature_extractor = clf_model.feature_extractor\n",
    "discriminator = clf_model.classifier\n",
    "\n",
    "critic = nn.Sequential(\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1)\n",
    ").to(device)\n",
    "\n",
    "half_batch = 32\n",
    "train_data = Image_data('./trainY.npy','./trainX.npy')\n",
    "source_dataset = train_data\n",
    "source_loader = DataLoader(source_dataset, batch_size=half_batch, drop_last=True,shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "target_dataset = test_data \n",
    "target_loader = DataLoader(target_dataset, batch_size=half_batch, drop_last=True,shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "critic_optim = torch.optim.Adam(critic.parameters(), lr=1e-4)\n",
    "clf_optim = torch.optim.Adam(clf_model.parameters(), lr=1e-4)\n",
    "clf_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 5+1):\n",
    "    batch_iterator = zip(loop_iterable(source_loader), loop_iterable(target_loader))\n",
    "\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for _ in trange(500, leave=False):\n",
    "        (source_x, source_y), (target_x) = next(batch_iterator)\n",
    "        # Train critic\n",
    "        set_requires_grad(feature_extractor, requires_grad=False)\n",
    "        set_requires_grad(critic, requires_grad=True)\n",
    "\n",
    "        source_x, target_x = source_x.to(device), target_x.to(device)\n",
    "        source_y = source_y.to(device)\n",
    "        source_y = source_y.long()\n",
    "        with torch.no_grad():\n",
    "            h_s = feature_extractor(source_x).data.view(source_x.shape[0], -1)\n",
    "            h_t = feature_extractor(target_x).data.view(target_x.shape[0], -1)\n",
    "        for _ in range(5):\n",
    "            gp = gradient_penalty(critic, h_s, h_t)\n",
    "\n",
    "            critic_s = critic(h_s)\n",
    "            critic_t = critic(h_t)\n",
    "            wasserstein_distance = critic_s.mean() - critic_t.mean()\n",
    "\n",
    "            critic_cost = -wasserstein_distance + 10*gp\n",
    "\n",
    "            critic_optim.zero_grad()\n",
    "            critic_cost.backward()\n",
    "            critic_optim.step()\n",
    "\n",
    "            total_loss += critic_cost.item()\n",
    "\n",
    "        # Train classifier\n",
    "        set_requires_grad(feature_extractor, requires_grad=True)\n",
    "        set_requires_grad(critic, requires_grad=False)\n",
    "        for _ in range(1):\n",
    "            source_features = feature_extractor(source_x).view(source_x.shape[0], -1)\n",
    "            target_features = feature_extractor(target_x).view(target_x.shape[0], -1)\n",
    "\n",
    "            source_preds = discriminator(source_features)\n",
    "            clf_loss = clf_criterion(source_preds, source_y)\n",
    "            wasserstein_distance = critic(source_features).mean() - critic(target_features).mean()\n",
    "\n",
    "            loss = clf_loss + 1 * wasserstein_distance\n",
    "            clf_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            clf_optim.step()\n",
    "\n",
    "    mean_loss = total_loss / (500 * 5)\n",
    "    tqdm.write(f'EPOCH {epoch:03d}: critic_loss={mean_loss:.4f}')\n",
    "    torch.save(clf_model.state_dict(), 'trained_models/wdgrl.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.Tensor(test_data)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "clf_model.eval()\n",
    "ans = []\n",
    "for idx ,img in enumerate(test_loader):\n",
    "    ans.append(clf_model(img))\n",
    "ans_final =[]\n",
    "for a in ans:\n",
    "    for pre in a:\n",
    "        pre = pre.tolist()\n",
    "        ans_final.append(pre.index(max(pre)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wdgrl.csv\",\"w\") as f:\n",
    "    print(\"id,label\", file = f)\n",
    "    for id,label in enumerate(ans_final):\n",
    "        print(\"{},{}\".format(id,label) ,file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
