{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout ,Activation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape: (32561, 106) \n",
      "Y_shape: (32561,)\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"X_train\",low_memory=False)\n",
    "X_test = pd.read_csv(\"X_test\",low_memory=False)\n",
    "Y = np.loadtxt(\"Y_train\",dtype=np.int,delimiter=',')\n",
    "Y_logistic = np.reshape(Y,(-1,1))\n",
    "Y = Y.flatten()\n",
    "\n",
    "X=np.array(X,dtype = float)\n",
    "X_test = np.array(X_test,dtype = float)\n",
    "\n",
    "\n",
    "X_mean = np.mean(X,axis =0,keepdims = True)\n",
    "X_std = np.std(X,axis = 0,keepdims = True)\n",
    "\n",
    "X = (X-X_mean)/X_std\n",
    "X_test = (X_test-X_mean)/X_std\n",
    "bias = np.ones((32561,1))\n",
    "biastest = np.ones((16281,1))\n",
    "X_logistic = np.concatenate((X,bias),axis = 1)\n",
    "X_test_logistic = np.concatenate((X_test,biastest),axis = 1)\n",
    "\n",
    "#bias = np.ones((32561,1))\n",
    "#biastest = np.ones((16281,1))\n",
    "#X = np.concatenate((X,bias),axis = 1)\n",
    "#X_test = np.concatenate((X_test,biastest),axis = 1)\n",
    "print(\"X_shape:\",X.shape,\"\\nY_shape:\",Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def _init_para(self):\n",
    "        self.W =np.ones((107,1))\n",
    "        \n",
    "    def fit(self, X, Y, valid=None, max_epoch=2000, lr=0.000001):\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        self._init_para()\n",
    "\n",
    "        for epoch in range(1, max_epoch+1):\n",
    "            \n",
    "            W_grad = -np.dot(X.T,(Y-self.predict(X)))\n",
    "            #print(W_grad.shape)\n",
    "            self.W = self.W -(lr*W_grad)\n",
    "            #print(self.W)\n",
    "\n",
    "            if epoch%100==0:\n",
    "                training_loss = self.loss(X,Y)\n",
    "                #accuracy = self.evaluate(X,Y)\n",
    "                print('[Epoch%5d] - training loss: %5f'%(epoch, training_loss))\n",
    "            \n",
    "    def sigmod(self,z):#   sigmod(z) = 1/(1+e^-z)\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def predict(self, X, test=False):\n",
    "        return self.sigmod(np.dot(X,self.W))\n",
    "\n",
    "    def loss(self, X, Y, pred=None):\n",
    "        predict_value = self.predict(X)\n",
    "        return -np.sum(Y*np.log(predict_value+0.00000000001)+(1-Y)*(np.log(1-predict_value+0.00000000001)))\n",
    "\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        pred=self.predict(X)\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i]>=0.5:\n",
    "                pred[i] = 1\n",
    "            else:\n",
    "                pred[i] =0\n",
    "            return np.mean(1-np.abs(pred-Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  100] - training loss: 49462.305997\n",
      "[Epoch  200] - training loss: 34862.255814\n",
      "[Epoch  300] - training loss: 26246.592140\n",
      "[Epoch  400] - training loss: 21088.055762\n",
      "[Epoch  500] - training loss: 18098.741507\n",
      "[Epoch  600] - training loss: 16220.104454\n",
      "[Epoch  700] - training loss: 14912.953588\n",
      "[Epoch  800] - training loss: 13944.184312\n",
      "[Epoch  900] - training loss: 13174.972544\n",
      "[Epoch 1000] - training loss: 12560.438753\n",
      "[Epoch 1100] - training loss: 12080.368998\n",
      "[Epoch 1200] - training loss: 11695.135036\n",
      "[Epoch 1300] - training loss: 11387.696303\n",
      "[Epoch 1400] - training loss: 11150.449163\n",
      "[Epoch 1500] - training loss: 10960.159789\n",
      "[Epoch 1600] - training loss: 10804.080743\n",
      "[Epoch 1700] - training loss: 10677.938866\n",
      "[Epoch 1800] - training loss: 10577.083859\n",
      "[Epoch 1900] - training loss: 10502.724193\n",
      "[Epoch 2000] - training loss: 10461.781170\n",
      "[Epoch 2100] - training loss: 10440.631333\n",
      "[Epoch 2200] - training loss: 10426.843834\n",
      "[Epoch 2300] - training loss: 10416.175234\n",
      "[Epoch 2400] - training loss: 10407.234370\n",
      "[Epoch 2500] - training loss: 10399.476319\n",
      "[Epoch 2600] - training loss: 10392.636363\n",
      "[Epoch 2700] - training loss: 10386.557762\n",
      "[Epoch 2800] - training loss: 10381.132580\n",
      "[Epoch 2900] - training loss: 10376.278474\n",
      "[Epoch 3000] - training loss: 10371.928337\n"
     ]
    }
   ],
   "source": [
    "logistic_model = logistic_regression()\n",
    "logistic_model.fit(X_logistic,Y_logistic,max_epoch = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generative_model():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def sigmoid(self,z):#   sigmod(z) = 1/(1+e^-z)\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        assert X.shape[0] == Y.shape[0]        \n",
    "        x1 = []#Y=1\n",
    "        x2 = []#Y=0\n",
    "        for i in range(X.shape[0]):\n",
    "            if Y[i]==1:\n",
    "                x1.append(X[i,:])\n",
    "            else:\n",
    "                x2.append(X[i,:])\n",
    "        x1 = np.array(x1,dtype = float)#~7000*106\n",
    "        x2 = np.array(x2,dtype = float)#~20000*106\n",
    "        \n",
    "        \n",
    "        self.X_mean_class1 = np.mean(x1,axis =0,keepdims = True)#1*106\n",
    "        self.X_mean_class2 = np.mean(x2,axis =0,keepdims = True)#1*106\n",
    "\n",
    "        self.X_len = x1.shape[0]+x2.shape[0]\n",
    "        self.Pc1 = (x1.shape[0]/X.shape[0])\n",
    "        self.Pc2 = (x2.shape[0]/X.shape[0])\n",
    "        \n",
    "        Sigma1 = np.dot((x1 - self.X_mean_class1).T,(x1 - self.X_mean_class1))/x1.shape[0]\n",
    "        Sigma2 = np.dot((x2 - self.X_mean_class2).T,(x2 - self.X_mean_class2))/x2.shape[0]\n",
    "        self.Sigma = self.Pc1*Sigma1 + self.Pc2*Sigma2\n",
    "        #print(\"Sigma1:\",Sigma1.shape,\"\\nSigma2:\",Sigma2.shape,\"\\nSigma:\",self.Sigma.shape)\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        #print(\"testx:\",test_X.shape)\n",
    "        sigma_inverse = np.linalg.pinv(self.Sigma)\n",
    "        #print(sigma_inverse)\n",
    "        w = np.dot((self.X_mean_class1-self.X_mean_class2),sigma_inverse)\n",
    "        #print(\"w=\",w)\n",
    "        b = (-0.5) * np.dot(np.dot(self.X_mean_class1, sigma_inverse), self.X_mean_class1.T) + (0.5) * np.dot(np.dot(self.X_mean_class2, sigma_inverse), self.X_mean_class2.T) + np.log(float(self.Pc1)/(self.Pc2))\n",
    "        z = np.dot(w, X_test.T) + b        \n",
    "        \n",
    "        return self.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = Generative_model()\n",
    "generative_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                5350      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                2040      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                1230      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 21        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 9,261\n",
      "Trainable params: 9,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnn_model = Sequential()\n",
    "dnn_model.add(Dense(50,input_dim = 106))\n",
    "dnn_model.add(Activation('relu') )\n",
    "dnn_model.add(Dropout(0.3))\n",
    "dnn_model.add(Dense(40))\n",
    "dnn_model.add(Activation('relu') )\n",
    "dnn_model.add(Dropout(0.3))\n",
    "dnn_model.add(Dense(30))\n",
    "dnn_model.add(Activation('relu') )\n",
    "dnn_model.add(Dropout(0.2))\n",
    "dnn_model.add(Dense(20))\n",
    "dnn_model.add(Activation('relu') )\n",
    "dnn_model.add(Dropout(0.2))\n",
    "dnn_model.add(Dense(1))\n",
    "dnn_model.add(Activation('sigmoid'))\n",
    "dnn_model.compile(loss = 'binary_crossentropy',optimizer= 'adam',metrics=['accuracy'])\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32398 samples, validate on 163 samples\n",
      "Epoch 1/40\n",
      "32398/32398 [==============================] - 2s 51us/step - loss: 0.4091 - acc: 0.8040 - val_loss: 0.2882 - val_acc: 0.8712\n",
      "Epoch 2/40\n",
      "32398/32398 [==============================] - 1s 40us/step - loss: 0.3534 - acc: 0.8395 - val_loss: 0.2857 - val_acc: 0.8650\n",
      "Epoch 3/40\n",
      "32398/32398 [==============================] - 1s 39us/step - loss: 0.3374 - acc: 0.8453 - val_loss: 0.2792 - val_acc: 0.8650\n",
      "Epoch 4/40\n",
      "32398/32398 [==============================] - 1s 39us/step - loss: 0.3298 - acc: 0.8493 - val_loss: 0.2760 - val_acc: 0.8712\n",
      "Epoch 5/40\n",
      "32398/32398 [==============================] - 1s 39us/step - loss: 0.3238 - acc: 0.8531 - val_loss: 0.2731 - val_acc: 0.8712\n",
      "Epoch 6/40\n",
      "32398/32398 [==============================] - 1s 39us/step - loss: 0.3202 - acc: 0.8545 - val_loss: 0.2739 - val_acc: 0.8589\n",
      "Epoch 7/40\n",
      "32398/32398 [==============================] - 1s 39us/step - loss: 0.3165 - acc: 0.8559 - val_loss: 0.2717 - val_acc: 0.8466\n",
      "Epoch 8/40\n",
      "32398/32398 [==============================] - 1s 40us/step - loss: 0.3155 - acc: 0.8564 - val_loss: 0.2729 - val_acc: 0.8589\n",
      "Epoch 9/40\n",
      "32398/32398 [==============================] - 1s 39us/step - loss: 0.3162 - acc: 0.8568 - val_loss: 0.2776 - val_acc: 0.8650\n",
      "Epoch 10/40\n",
      "32398/32398 [==============================] - 1s 41us/step - loss: 0.3121 - acc: 0.8579 - val_loss: 0.2734 - val_acc: 0.8528\n",
      "Epoch 11/40\n",
      "32398/32398 [==============================] - 1s 40us/step - loss: 0.3104 - acc: 0.8584 - val_loss: 0.2667 - val_acc: 0.8650\n",
      "Epoch 12/40\n",
      "32398/32398 [==============================] - 1s 39us/step - loss: 0.3115 - acc: 0.8593 - val_loss: 0.2808 - val_acc: 0.8528\n",
      "Epoch 13/40\n",
      "32398/32398 [==============================] - 1s 39us/step - loss: 0.3088 - acc: 0.8591 - val_loss: 0.2651 - val_acc: 0.8650\n",
      "Epoch 14/40\n",
      "32398/32398 [==============================] - 1s 40us/step - loss: 0.3094 - acc: 0.8598 - val_loss: 0.2668 - val_acc: 0.8712\n",
      "Epoch 15/40\n",
      "32398/32398 [==============================] - 1s 38us/step - loss: 0.3078 - acc: 0.8592 - val_loss: 0.2721 - val_acc: 0.8650\n",
      "Epoch 16/40\n",
      "32398/32398 [==============================] - 1s 37us/step - loss: 0.3051 - acc: 0.8611 - val_loss: 0.2700 - val_acc: 0.8466\n",
      "Epoch 17/40\n",
      "32398/32398 [==============================] - 1s 38us/step - loss: 0.3059 - acc: 0.8605 - val_loss: 0.2690 - val_acc: 0.8589\n",
      "Epoch 18/40\n",
      "32398/32398 [==============================] - 1s 36us/step - loss: 0.3059 - acc: 0.8601 - val_loss: 0.2797 - val_acc: 0.8528\n",
      "Epoch 19/40\n",
      "32398/32398 [==============================] - 1s 35us/step - loss: 0.3056 - acc: 0.8599 - val_loss: 0.2852 - val_acc: 0.8528\n",
      "Epoch 20/40\n",
      "32398/32398 [==============================] - 1s 36us/step - loss: 0.3028 - acc: 0.8624 - val_loss: 0.2676 - val_acc: 0.8650\n",
      "Epoch 21/40\n",
      "32398/32398 [==============================] - 1s 36us/step - loss: 0.3031 - acc: 0.8624 - val_loss: 0.2695 - val_acc: 0.8528\n",
      "Epoch 22/40\n",
      "32398/32398 [==============================] - 1s 36us/step - loss: 0.3032 - acc: 0.8632 - val_loss: 0.2687 - val_acc: 0.8528\n",
      "Epoch 23/40\n",
      "32398/32398 [==============================] - 1s 36us/step - loss: 0.3012 - acc: 0.8631 - val_loss: 0.2744 - val_acc: 0.8589\n",
      "Epoch 24/40\n",
      "32398/32398 [==============================] - 1s 36us/step - loss: 0.3013 - acc: 0.8627 - val_loss: 0.2725 - val_acc: 0.8773\n",
      "Epoch 25/40\n",
      "32398/32398 [==============================] - 1s 36us/step - loss: 0.2983 - acc: 0.8630 - val_loss: 0.2770 - val_acc: 0.8589\n",
      "Epoch 26/40\n",
      "32398/32398 [==============================] - 1s 36us/step - loss: 0.3009 - acc: 0.8633 - val_loss: 0.2657 - val_acc: 0.8589\n",
      "Epoch 27/40\n",
      "32398/32398 [==============================] - 1s 36us/step - loss: 0.2993 - acc: 0.8651 - val_loss: 0.2667 - val_acc: 0.8650\n",
      "Epoch 28/40\n",
      "32398/32398 [==============================] - 1s 36us/step - loss: 0.2983 - acc: 0.8657 - val_loss: 0.2646 - val_acc: 0.8834\n",
      "Epoch 29/40\n",
      "32398/32398 [==============================] - 1s 36us/step - loss: 0.2980 - acc: 0.8650 - val_loss: 0.2725 - val_acc: 0.8773\n",
      "Epoch 30/40\n",
      "32398/32398 [==============================] - 1s 37us/step - loss: 0.2974 - acc: 0.8646 - val_loss: 0.2679 - val_acc: 0.8650\n",
      "Epoch 31/40\n",
      "32398/32398 [==============================] - 1s 37us/step - loss: 0.2980 - acc: 0.8649 - val_loss: 0.2701 - val_acc: 0.8773\n",
      "Epoch 32/40\n",
      "32398/32398 [==============================] - 1s 37us/step - loss: 0.2976 - acc: 0.8652 - val_loss: 0.2795 - val_acc: 0.8712\n",
      "Epoch 33/40\n",
      "32398/32398 [==============================] - 1s 37us/step - loss: 0.2962 - acc: 0.8649 - val_loss: 0.2666 - val_acc: 0.8773\n",
      "Epoch 34/40\n",
      "32398/32398 [==============================] - 1s 37us/step - loss: 0.2972 - acc: 0.8660 - val_loss: 0.2680 - val_acc: 0.8773\n",
      "Epoch 35/40\n",
      "32398/32398 [==============================] - 1s 38us/step - loss: 0.2964 - acc: 0.8659 - val_loss: 0.2620 - val_acc: 0.8712\n",
      "Epoch 36/40\n",
      "32398/32398 [==============================] - 1s 37us/step - loss: 0.2960 - acc: 0.8663 - val_loss: 0.2730 - val_acc: 0.8650\n",
      "Epoch 37/40\n",
      "32398/32398 [==============================] - 1s 37us/step - loss: 0.2952 - acc: 0.8668 - val_loss: 0.2708 - val_acc: 0.8712\n",
      "Epoch 38/40\n",
      "32398/32398 [==============================] - 1s 38us/step - loss: 0.2961 - acc: 0.8658 - val_loss: 0.2604 - val_acc: 0.8773\n",
      "Epoch 39/40\n",
      "32398/32398 [==============================] - 1s 38us/step - loss: 0.2946 - acc: 0.8668 - val_loss: 0.2639 - val_acc: 0.8589\n",
      "Epoch 40/40\n",
      "32398/32398 [==============================] - 1s 38us/step - loss: 0.2908 - acc: 0.8666 - val_loss: 0.2696 - val_acc: 0.8528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11997d208>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.fit(x = X,y = Y,epochs =40,validation_split= 0.005,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_model = SVC(kernel='linear', C=1, gamma='auto',verbose  =1)\n",
    "svc_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0396           30.50s\n",
      "         2           0.9905           29.14s\n",
      "         3           0.9508           28.86s\n",
      "         4           0.9179           28.49s\n",
      "         5           0.8903           28.23s\n",
      "         6           0.8669           28.29s\n",
      "         7           0.8445           28.27s\n",
      "         8           0.8271           27.98s\n",
      "         9           0.8113           27.97s\n",
      "        10           0.7971           27.87s\n",
      "        20           0.7115           26.44s\n",
      "        30           0.6690           24.28s\n",
      "        40           0.6436           22.47s\n",
      "        50           0.6275           20.66s\n",
      "        60           0.6156           19.15s\n",
      "        70           0.6070           17.62s\n",
      "        80           0.5985           16.25s\n",
      "        90           0.5914           15.15s\n",
      "       100           0.5853           14.15s\n",
      "       200           0.5527            6.34s\n",
      "       300           0.5324            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=1,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosting_model = GradientBoostingClassifier(n_estimators = 300,verbose  =1)\n",
    "boosting_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16281, 1) (16281, 1) (16281, 1) (16281, 1) (16281, 1)\n",
      "(16281, 1)\n"
     ]
    }
   ],
   "source": [
    "predict_boosting = boosting_model.predict_proba(X_test)\n",
    "predict_logistic = logistic_model.predict(X_test_logistic)\n",
    "predict_dnn = dnn_model.predict(X_test)\n",
    "predict_generative = generative_model.predict(X_test)\n",
    "predict_svc =svc_model.predict(X_test)\n",
    "predict_svc = np.reshape(predict_svc,(-1,1))\n",
    "predict_logistic = np.reshape(predict_logistic,(-1,1))\n",
    "predict_generative = np.reshape(predict_generative,(-1,1))\n",
    "predict_boosting = np.reshape(predict_generative,(-1,1))\n",
    "print(predict_logistic.shape,predict_dnn.shape,predict_svc.shape,predict_generative.shape,predict_boosting.shape)\n",
    "finialresult = (predict_logistic*0.2 + predict_dnn*0.3 + predict_generative*0.1 + predict_svc*0.1+predict_boosting*0.3)\n",
    "print(finialresult.shape)\n",
    "for i in range(len(finialresult)):\n",
    "    if finialresult[i]<0.5:\n",
    "        finialresult[i] = 0\n",
    "    else:\n",
    "        finialresult[i] = 1\n",
    "finialresult =np.array(finialresult,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open('emsemble.csv', 'w') as f:\n",
    "        print('id,label', file=f)\n",
    "        for (i, p) in enumerate(finialresult) :\n",
    "            print('{},{}'.format(i+1, p[0]), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
