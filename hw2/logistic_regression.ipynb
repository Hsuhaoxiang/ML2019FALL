{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape: (32561, 107) \n",
      "Y_shape: (32561, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "X = pd.read_csv(\"X_train\",low_memory=False)\n",
    "\n",
    "X_test = pd.read_csv(\"X_test\",low_memory=False)\n",
    "X=np.array(X,dtype = float)\n",
    "X_test = np.array(X_test,dtype = float)\n",
    "\n",
    "\n",
    "X_mean = np.mean(X,axis =0,keepdims = True)\n",
    "X_std = np.std(X,axis = 0,keepdims = True)\n",
    "\n",
    "X = (X-X_mean)/X_std\n",
    "X_test = (X_test-X_mean)/X_std\n",
    "\n",
    "\n",
    "\n",
    "#Y = pd.read_csv(\"Y_train\",low_memory=False)\n",
    "#Y = np.array(Y,dtype = float)\n",
    "Y = np.loadtxt(\"Y_train\",dtype=np.float,delimiter=',')\n",
    "\n",
    "Y = np.reshape(Y,(-1,1))\n",
    "\n",
    "bias = np.ones((32561,1))\n",
    "biastest = np.ones((16281,1))\n",
    "X = np.concatenate((X,bias),axis = 1)\n",
    "X_test = np.concatenate((X_test,biastest),axis = 1)\n",
    "print(\"X_shape:\",X.shape,\"\\nY_shape:\",Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def _init_para(self):\n",
    "        self.W =np.ones((107,1))\n",
    "        \n",
    "    def fit(self, X, Y, valid=None, max_epoch=2000, lr=0.000001):\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        self._init_para()\n",
    "\n",
    "        for epoch in range(1, max_epoch+1):\n",
    "            \n",
    "            W_grad = -np.dot(X.T,(Y-self.predict(X)))\n",
    "            #print(W_grad.shape)\n",
    "            self.W = self.W -(lr*W_grad)\n",
    "            #print(self.W)\n",
    "\n",
    "            if epoch%100==0:\n",
    "                training_loss = self.loss(X,Y)\n",
    "                #accuracy = self.evaluate(X,Y)\n",
    "                print('[Epoch%5d] - training loss: %5f'%(epoch, training_loss))\n",
    "            \n",
    "    def sigmod(self,z):#   sigmod(z) = 1/(1+e^-z)\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def predict(self, X, test=False):\n",
    "        return self.sigmod(np.dot(X,self.W))\n",
    "\n",
    "    def loss(self, X, Y, pred=None):\n",
    "        predict_value = self.predict(X)\n",
    "        return -np.sum(Y*np.log(predict_value+0.00000000001)+(1-Y)*(np.log(1-predict_value+0.00000000001)))\n",
    "\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        pred=self.predict(X)\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i]>=0.5:\n",
    "                pred[i] = 1\n",
    "            else:\n",
    "                pred[i] =0\n",
    "            return np.mean(1-np.abs(pred-Y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  100] - training loss: 49462.305997\n",
      "[Epoch  200] - training loss: 34862.255814\n",
      "[Epoch  300] - training loss: 26246.592140\n",
      "[Epoch  400] - training loss: 21088.055762\n",
      "[Epoch  500] - training loss: 18098.741507\n",
      "[Epoch  600] - training loss: 16220.104454\n",
      "[Epoch  700] - training loss: 14912.953588\n",
      "[Epoch  800] - training loss: 13944.184312\n",
      "[Epoch  900] - training loss: 13174.972544\n",
      "[Epoch 1000] - training loss: 12560.438753\n",
      "[Epoch 1100] - training loss: 12080.368998\n",
      "[Epoch 1200] - training loss: 11695.135036\n",
      "[Epoch 1300] - training loss: 11387.696303\n",
      "[Epoch 1400] - training loss: 11150.449163\n",
      "[Epoch 1500] - training loss: 10960.159789\n",
      "[Epoch 1600] - training loss: 10804.080743\n",
      "[Epoch 1700] - training loss: 10677.938866\n",
      "[Epoch 1800] - training loss: 10577.083859\n",
      "[Epoch 1900] - training loss: 10502.724193\n",
      "[Epoch 2000] - training loss: 10461.781170\n",
      "[Epoch 2100] - training loss: 10440.631333\n",
      "[Epoch 2200] - training loss: 10426.843834\n",
      "[Epoch 2300] - training loss: 10416.175234\n",
      "[Epoch 2400] - training loss: 10407.234370\n",
      "[Epoch 2500] - training loss: 10399.476319\n",
      "[Epoch 2600] - training loss: 10392.636363\n",
      "[Epoch 2700] - training loss: 10386.557762\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f9b37933a0b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6c844b6b06a7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, valid, max_epoch, lr)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mW_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;31m#print(W_grad.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mW_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6c844b6b06a7>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, test)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = logistic_regression()\n",
    "model.fit(X,Y,max_epoch = 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(predict.shape)\n",
    "for i in range(len(predict)):\n",
    "    if predict[i]<0.5:\n",
    "        predict[i] = 0\n",
    "    else:\n",
    "        predict[i] = 1\n",
    "predict =np.array(predict,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open('logistic_regression.csv', 'w') as f:\n",
    "        print('id,label', file=f)\n",
    "        for (i, p) in enumerate(predict) :\n",
    "            print('{},{}'.format(i+1, p[0]), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
