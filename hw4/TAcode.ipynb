{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: Loss : 0.00001335\n",
      "Epoch 001: Loss : 0.00001254\n",
      "Epoch 002: Loss : 0.00000737\n",
      "Epoch 003: Loss : 0.00000646\n",
      "Epoch 004: Loss : 0.00000505\n",
      "Epoch 005: Loss : 0.00000430\n",
      "Epoch 006: Loss : 0.00000439\n",
      "Epoch 007: Loss : 0.00000408\n",
      "Epoch 008: Loss : 0.00000414\n",
      "Epoch 009: Loss : 0.00000401\n",
      "Epoch 010: Loss : 0.00000328\n",
      "Epoch 011: Loss : 0.00000296\n",
      "Epoch 012: Loss : 0.00000324\n",
      "Epoch 013: Loss : 0.00000280\n",
      "Epoch 014: Loss : 0.00000320\n",
      "Epoch 015: Loss : 0.00000407\n",
      "Epoch 016: Loss : 0.00000252\n",
      "Epoch 017: Loss : 0.00000268\n",
      "Epoch 018: Loss : 0.00000252\n",
      "Epoch 019: Loss : 0.00000301\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # define: encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "          nn.Conv2d(3, 4, 3, 2, 1),\n",
    "          nn.Conv2d(4, 8, 3, 2, 1),\n",
    "          nn.Conv2d(8, 16, 3, 2, 1),\n",
    "        )\n",
    "\n",
    "        # define: decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "          nn.ConvTranspose2d(16, 8, 2, 2),\n",
    "          nn.ConvTranspose2d(8, 3, 2, 2),\n",
    "          nn.Tanh(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        encoded = self.encoder(x)\n",
    "        #print(encoded.shape)\n",
    "        decoded = self.decoder(encoded)\n",
    "\n",
    "        # Total AE: return latent & reconstruct\n",
    "        return encoded, decoded\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # detect is gpu available.\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "\n",
    "    autoencoder = Autoencoder()\n",
    "    \n",
    "    # load data and normalize to [-1, 1]\n",
    "    trainX = np.load('./trainX.npy')\n",
    "    trainX = np.transpose(trainX, (0, 3, 1, 2)) / 255. * 2 - 1\n",
    "    trainX = torch.Tensor(trainX)\n",
    "\n",
    "    # if use_gpu, send model / data to GPU.\n",
    "    if use_gpu:\n",
    "        autoencoder.cuda()\n",
    "        trainX = trainX.cuda()\n",
    "\n",
    "    # Dataloader: train shuffle = True\n",
    "    train_dataloader = DataLoader(trainX, batch_size=32, shuffle=True)\n",
    "    test_dataloader = DataLoader(trainX, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "    # We set criterion : L1 loss (or Mean Absolute Error, MAE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "    # Now, we train 20 epochs.\n",
    "    for epoch in range(20):\n",
    "\n",
    "        cumulate_loss = 0\n",
    "        for x in train_dataloader:\n",
    "            \n",
    "            latent, reconstruct = autoencoder(x)\n",
    "            loss = criterion(reconstruct, x)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            cumulate_loss = loss.item() * x.shape[0]\n",
    "\n",
    "        print(f'Epoch { \"%03d\" % epoch }: Loss : { \"%.8f\" % (cumulate_loss / trainX.shape[0])}')\n",
    "\n",
    "\n",
    "    # Collect the latents and stdardize it.\n",
    "    latents = []\n",
    "    reconstructs = []\n",
    "    for x in test_dataloader:\n",
    "\n",
    "        latent, reconstruct = autoencoder(x)\n",
    "        latents.append(latent.cpu().detach().numpy())\n",
    "        reconstructs.append(reconstruct.cpu().detach().numpy())\n",
    "\n",
    "    latents = np.concatenate(latents, axis=0).reshape([9000, -1])\n",
    "    latents = (latents - np.mean(latents, axis=0)) / np.std(latents, axis=0)\n",
    "\n",
    "\n",
    "    # Use PCA to lower dim of latents and use K-means to clustering.\n",
    "    latents = PCA(n_components=32,whiten = True).fit_transform(latents)\n",
    "    result = KMeans(n_clusters = 2).fit(latents).labels_\n",
    "\n",
    "    # We know first 5 labels are zeros, it's a mechanism to check are your answers\n",
    "    # need to be flipped or not.\n",
    "    if np.sum(result[:5]) >= 3:\n",
    "        result = 1 - result\n",
    "\n",
    "\n",
    "    # Generate your submission\n",
    "    df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
    "    df.to_csv('haha_submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
