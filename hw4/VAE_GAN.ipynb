{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2\n",
    "import argparse\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "# other library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "# PyTorch library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data \n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # define: encoder\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        # generate mean var\n",
    "        self.fnc = nn.Sequential(\n",
    "            nn.Linear(576, 256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x1 = self.conv1(x)\n",
    "        #print(x1.shape)\n",
    "        x2 = self.conv2(x1)\n",
    "        #print(x2.shape)\n",
    "        x3 = self.conv3(x2)\n",
    "        #print(x3.shape)\n",
    "        latent = x3.view(len(x3), -1)\n",
    "        #print(latent.shape)\n",
    "        predict = self.fnc(latent)\n",
    "        return predict\n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # define: encoder\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # generate mean var\n",
    "        self.fc1 = nn.Linear(1600, 512)\n",
    "        self.fc2 = nn.Linear(1600, 512)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # define: decoder\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 5, 2, dilation=2),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 16, 4, 2, dilation=2),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 3, 6, 1, dilation=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        #print(x1.shape)\n",
    "        x2 = self.conv2(x1)\n",
    "        #print(x2.shape)\n",
    "        x3 = self.conv3(x2)\n",
    "        #print(x3.shape)\n",
    "        return x3\n",
    "    \n",
    "    def bottleneck(self,latent):\n",
    "        latent = latent.view(len(latent), -1)\n",
    "        mean = self.fc1(latent)\n",
    "        var = self.fc2(latent)\n",
    "        z = self.reparameterize(mean,var)\n",
    "        #print(z.shape)\n",
    "        return z, mean, var\n",
    "    \n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mean, var):\n",
    "        std = var.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mean.size())\n",
    "        z = mean + std * esp\n",
    "        return z\n",
    "    \n",
    "    \n",
    "\n",
    "    def decoder(self, z):\n",
    "        z =  z.view(-1, 128, 2, 2)\n",
    "        x4 = self.decoder1(z)\n",
    "        #print(x4.shape)\n",
    "        x5 = self.decoder2(x4)\n",
    "        #print(x5.shape)\n",
    "        x6 = self.decoder3(x5)\n",
    "        #print(x6.shape)\n",
    "        #print(x6)\n",
    "        return x6\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        z , mean, var = self.bottleneck(latent) \n",
    "        rec_ = self.decoder(z)\n",
    "        #print(\"fake\",rec_.shape)\n",
    "        return rec_, mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# detect is gpu available.\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    device =torch.device('cuda:0')\n",
    "else:\n",
    "    device =torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# load data and normalize to [-1, 1]\n",
    "trainX = np.load('./trainX.npy')\n",
    "print(trainX.shape)\n",
    "trainX = np.transpose(trainX, (0, 3, 1, 2))/ 255.*2 -1\n",
    "trainX = torch.Tensor(trainX)\n",
    "\n",
    "\n",
    "# if use_gpu, send model / data to GPU.\n",
    "if use_gpu:\n",
    "    autoencoder.cuda()\n",
    "    trainX = trainX.cuda()\n",
    "\n",
    "# Dataloader: train shuffle = True\n",
    "train_dataloader = DataLoader(trainX, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(trainX, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    # BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD, BCE, KLD\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "#     BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    \n",
    "    loss = nn.L1Loss(reduction='sum')\n",
    "#     MSE = F.mse_loss(recon_x, x, size_average=False)\n",
    "    l1_loss = loss(recon_x, x)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return l1_loss + KLD, KLD, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    # print \"real_data: \", real_data.size(), fake_data.size()\n",
    "    alpha = torch.rand(real_data.shape[0], 1)\n",
    "    alpha = alpha.expand(real_data.shape[0], round(real_data.nelement()/real_data.shape[0])).contiguous().view(real_data.shape[0], 3, 32, 32)\n",
    "    alpha = alpha.cuda(gpu) if use_cuda else alpha\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    \n",
    "    if use_cuda:\n",
    "        interpolates = interpolates.cuda(gpu)\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).cuda(gpu) if use_cuda else torch.ones(\n",
    "                                  disc_interpolates.size()),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * 10\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (fc1): Linear(in_features=1600, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=1600, out_features=512, bias=True)\n",
      "  (decoder1): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), dilation=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (decoder2): Sequential(\n",
      "    (0): ConvTranspose2d(64, 16, kernel_size=(4, 4), stride=(2, 2), dilation=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (decoder3): Sequential(\n",
      "    (0): ConvTranspose2d(16, 3, kernel_size=(6, 6), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (fnc): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = VAE()\n",
    "netD = Discriminator()\n",
    "print(netG)\n",
    "print(netD)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    gpu = 0\n",
    "if use_cuda:\n",
    "    Gnet = Gnet.to(device)\n",
    "    Dnet = Dnet.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1ensor(0.2626, grad_fn=<MulBackward0>)\n",
      "2ensor(0.4061, grad_fn=<MulBackward0>)\n",
      "3ensor(0.3649, grad_fn=<MulBackward0>)\n",
      "4ensor(0.2662, grad_fn=<MulBackward0>)\n",
      "5ensor(0.2437, grad_fn=<MulBackward0>)\n",
      "6ensor(0.1523, grad_fn=<MulBackward0>)\n",
      "7ensor(0.2276, grad_fn=<MulBackward0>)\n",
      "8ensor(0.0419, grad_fn=<MulBackward0>)\n",
      "9ensor(0.3675, grad_fn=<MulBackward0>)\n",
      "10nsor(0.1044, grad_fn=<MulBackward0>)\n",
      "11nsor(0.1611, grad_fn=<MulBackward0>)\n",
      "12nsor(0.0641, grad_fn=<MulBackward0>)\n",
      "13nsor(0.1988, grad_fn=<MulBackward0>)\n",
      "14nsor(0.0642, grad_fn=<MulBackward0>)\n",
      "15nsor(0.4570, grad_fn=<MulBackward0>)\n",
      "16nsor(0.2234, grad_fn=<MulBackward0>)\n",
      "17nsor(0.1021, grad_fn=<MulBackward0>)\n",
      "18nsor(0.1947, grad_fn=<MulBackward0>)\n",
      "19nsor(0.1739, grad_fn=<MulBackward0>)\n",
      "20nsor(0.0921, grad_fn=<MulBackward0>)\n",
      "21nsor(1.6655, grad_fn=<MulBackward0>)\n",
      "22nsor(0.7257, grad_fn=<MulBackward0>)\n",
      "23nsor(0.7637, grad_fn=<MulBackward0>)\n",
      "24nsor(0.2066, grad_fn=<MulBackward0>)\n",
      "25nsor(0.5460, grad_fn=<MulBackward0>)\n",
      "26nsor(0.7222, grad_fn=<MulBackward0>)\n",
      "27nsor(0.4349, grad_fn=<MulBackward0>)\n",
      "28nsor(0.2427, grad_fn=<MulBackward0>)\n",
      "29nsor(0.2835, grad_fn=<MulBackward0>)\n",
      "30nsor(0.1446, grad_fn=<MulBackward0>)\n",
      "31nsor(0.5086, grad_fn=<MulBackward0>)\n",
      "32nsor(0.2565, grad_fn=<MulBackward0>)\n",
      "33nsor(0.1304, grad_fn=<MulBackward0>)\n",
      "34nsor(0.4991, grad_fn=<MulBackward0>)\n",
      "35nsor(0.1150, grad_fn=<MulBackward0>)\n",
      "36nsor(0.2084, grad_fn=<MulBackward0>)\n",
      "37nsor(0.0670, grad_fn=<MulBackward0>)\n",
      "38nsor(0.1218, grad_fn=<MulBackward0>)\n",
      "39nsor(0.0455, grad_fn=<MulBackward0>)\n",
      "40nsor(0.0844, grad_fn=<MulBackward0>)\n",
      "41nsor(0.0441, grad_fn=<MulBackward0>)\n",
      "42nsor(0.3198, grad_fn=<MulBackward0>)\n",
      "43nsor(0.1294, grad_fn=<MulBackward0>)\n",
      "44nsor(0.1465, grad_fn=<MulBackward0>)\n",
      "45nsor(0.0562, grad_fn=<MulBackward0>)\n",
      "46nsor(0.1402, grad_fn=<MulBackward0>)\n",
      "47nsor(0.0686, grad_fn=<MulBackward0>)\n",
      "48nsor(0.2878, grad_fn=<MulBackward0>)\n",
      "49nsor(0.1803, grad_fn=<MulBackward0>)\n",
      "50nsor(0.1144, grad_fn=<MulBackward0>)\n",
      "51nsor(0.1508, grad_fn=<MulBackward0>)\n",
      "52nsor(0.0616, grad_fn=<MulBackward0>)\n",
      "53nsor(0.0904, grad_fn=<MulBackward0>)\n",
      "54nsor(0.3279, grad_fn=<MulBackward0>)\n",
      "55nsor(0.0705, grad_fn=<MulBackward0>)\n",
      "tensor(0.0566, grad_fn=<MulBackward0>)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2d82de011345>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mG_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0moptimizerG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We set criterion : L1 loss (or Mean Absolute Error, MAE)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "\n",
    "if use_cuda:\n",
    "    one = one.cuda(gpu)\n",
    "    mone = mone.cuda(gpu)\n",
    "\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "for iteration in range(100):\n",
    "    print(iteration)\n",
    "    G_loss = 0\n",
    "    D_loss = 0\n",
    "    ############################\n",
    "    # (1) Update D network\n",
    "    ###########################\n",
    "    for idx, image in enumerate(train_dataloader):\n",
    "            d_loss = 0\n",
    "            g_loss = 0\n",
    "            for p in netD.parameters():  # reset requires_grad\n",
    "                p.requires_grad = True  # they are set to False below in netG update\n",
    "\n",
    "            _data = image\n",
    "            netD.zero_grad()\n",
    "\n",
    "\n",
    "            # import torchvision\n",
    "            # filename = os.path.join(\"test_train_data\", str(iteration) + str(i) + \".jpg\")\n",
    "            # torchvision.utils.save_image(real_data, filename)\n",
    "\n",
    "            D_real = netD(image)\n",
    "            D_real = D_real.mean()\n",
    "            D_real.backward(mone)\n",
    "\n",
    "            # train with fake\n",
    "\n",
    "            fake = autograd.Variable(netG(image)[0].data)\n",
    "            inputv = fake\n",
    "            D_fake = netD(inputv)\n",
    "            D_fake = D_fake.mean()\n",
    "            D_fake.backward(one)\n",
    "\n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD, image.data, fake.data)\n",
    "            d_loss=gradient_penalty\n",
    "            D_loss+=gradient_penalty\n",
    "            gradient_penalty.backward()\n",
    "\n",
    "            # print \"gradien_penalty: \", gradient_penalty\n",
    "\n",
    "            D_cost = D_fake - D_real + gradient_penalty\n",
    "            Wasserstein_D = D_real - D_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network\n",
    "            ###########################\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False  # to avoid computation\n",
    "            netG.zero_grad()\n",
    "\n",
    "\n",
    "            fake = netG(image)[0]\n",
    "            G = netD(fake)\n",
    "            G = G.mean()\n",
    "            G.backward(mone)\n",
    "            G_cost = -G\n",
    "            optimizerG.step()\n",
    "            print(d_loss,end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_space finish\n",
      "(9000, 512)\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 9000 samples in 0.007s...\n",
      "[t-SNE] Computed neighbors for 9000 samples in 4.172s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 9000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 9000\n",
      "[t-SNE] Mean sigma: 1.197040\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 92.095146\n",
      "[t-SNE] KL divergence after 1000 iterations: 2.123506\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Collect the latents and stdardize it.\n",
    "latents = []\n",
    "latent_sapce = []\n",
    "for x in test_dataloader:\n",
    "    _,mu,var = netG(x)\n",
    "    mu = mu.detach().cpu().numpy()\n",
    "    for i in range(mu.shape[0]):\n",
    "        latent_sapce.append(mu[i])\n",
    "        \n",
    "print('latent_space finish')\n",
    "latent_space = np.asarray(latent_sapce)\n",
    "\n",
    "print(latent_space.shape)\n",
    "latents = (latent_space - np.mean(latent_space, axis=0)) / np.std(latent_space, axis=0)\n",
    "\n",
    "# Use PCA to lower dim of latents and use K-means to clustering.\n",
    "pca = PCA(n_components=32, copy=False, whiten=True, svd_solver='full')\n",
    "latent_vec = pca.fit_transform(latents)\n",
    "latent_vec = TSNE(n_components = 3,verbose=1).fit_transform(latent_vec)\n",
    "result = KMeans(n_clusters=2, random_state=2, max_iter=1000).fit(latent_vec).labels_\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "latents = PCA(n_components=16).fit_transform(latents)\n",
    "result = KMeans(n_clusters = 2).fit(latents).labels_\n",
    "\"\"\"\n",
    "# We know first 5 labels are zeros, it's a mechanism to check are your answers\n",
    "# need to be flipped or not.\n",
    "print(np.sum(result[:5]))\n",
    "if np.sum(result[:5]) >= 3:\n",
    "    result = 1 - result\n",
    "\"\"\"\"\n",
    "if np.sum(result[:5]) != 0 or np.sum(result[:5])!=5:\n",
    "    print(\"redo\")\n",
    "\"\"\"\n",
    "# Generate your submission\n",
    "df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
    "df.to_csv('baseline_11_15_03_30.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
