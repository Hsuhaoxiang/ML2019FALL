{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2\n",
    "import argparse\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "# other library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "# PyTorch library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data \n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # define: encoder\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        # generate mean var\n",
    "        self.fnc = nn.Sequential(\n",
    "            nn.Linear(1600, 512),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        #print(x1.shape)\n",
    "        x2 = self.conv2(x1)\n",
    "        #print(x2.shape)\n",
    "        x3 = self.conv3(x2)\n",
    "        #print(x3.shape)\n",
    "        latent = x3.view(len(latent), -1)\n",
    "        predict = self.fnc(latent)\n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # define: encoder\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # generate mean var\n",
    "        self.fc1 = nn.Linear(1600, 256)\n",
    "        self.fc2 = nn.Linear(1600, 256)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # define: decoder\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 5, 2, dilation=2),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 8, 4, 2, dilation=2),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 3, 6, 1, dilation=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        #print(x1.shape)\n",
    "        x2 = self.conv2(x1)\n",
    "        #print(x2.shape)\n",
    "        x3 = self.conv3(x2)\n",
    "        #print(x3.shape)\n",
    "        return x3\n",
    "    \n",
    "    def bottleneck(self,latent):\n",
    "        latent = latent.view(len(latent), -1)\n",
    "        mean = self.fc1(latent)\n",
    "        var = self.fc2(latent)\n",
    "        z = self.reparameterize(mean,var)\n",
    "        #print(z.shape)\n",
    "        return z, mean, var\n",
    "    \n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mean, var):\n",
    "        std = var.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mean.size())\n",
    "        z = mean + std * esp\n",
    "        return z\n",
    "    \n",
    "    \n",
    "\n",
    "    def decoder(self, z):\n",
    "        z =  z.view(-1, 32, 2, 2)\n",
    "        x4 = self.decoder1(z)\n",
    "        #print(x4.shape)\n",
    "        x5 = self.decoder2(x4)\n",
    "        #print(x5.shape)\n",
    "        x6 = self.decoder3(x5)\n",
    "        #print(x6.shape)\n",
    "        #print(x6)\n",
    "        return x6\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        z , mean, var = self.bottleneck(latent) \n",
    "        rec_ = self.decoder(z)\n",
    "        return rec_, mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# detect is gpu available.\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    device =torch.device('cuda:0')\n",
    "else:\n",
    "    device =torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# load data and normalize to [-1, 1]\n",
    "trainX = np.load('./trainX.npy')\n",
    "print(trainX.shape)\n",
    "trainX = np.transpose(trainX, (0, 3, 1, 2))/ 255.*2 -1\n",
    "trainX = torch.Tensor(trainX)\n",
    "\n",
    "\n",
    "# if use_gpu, send model / data to GPU.\n",
    "if use_gpu:\n",
    "    autoencoder.cuda()\n",
    "    trainX = trainX.cuda()\n",
    "\n",
    "# Dataloader: train shuffle = True\n",
    "train_dataloader = DataLoader(trainX, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(trainX, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    # BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD, BCE, KLD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "#     BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    \n",
    "    loss = nn.L1Loss(reduction='sum')\n",
    "#     MSE = F.mse_loss(recon_x, x, size_average=False)\n",
    "    l1_loss = loss(recon_x, x)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return l1_loss + KLD, KLD, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (fc1): Linear(in_features=1600, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=1600, out_features=256, bias=True)\n",
      "  (decoder1): Sequential(\n",
      "    (0): ConvTranspose2d(32, 16, kernel_size=(5, 5), stride=(2, 2), dilation=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (decoder2): Sequential(\n",
      "    (0): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), dilation=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (decoder3): Sequential(\n",
      "    (0): ConvTranspose2d(8, 3, kernel_size=(6, 6), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace)\n",
      "  )\n",
      "  (fnc): Sequential(\n",
      "    (0): Linear(in_features=1600, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Gnet = VAE()\n",
    "Dnet = Discriminator()\n",
    "print(Gnet)\n",
    "print(Dnet)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    gpu = 0\n",
    "if use_cuda:\n",
    "    Gnet = Gnet.to(device)\n",
    "    Dnet = Dnet.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | 8992/9024] loss: 9876.880859380\n",
      "  Training  | Loss:39293.9617 \n",
      "\n",
      "\n",
      "[Epoch 2 | 8992/9024] loss: 7622.386718755\n",
      "  Training  | Loss:34271.5282 \n",
      "\n",
      "\n",
      "[Epoch 3 | 8992/9024] loss: 9940.916992198\n",
      "  Training  | Loss:33068.4788 \n",
      "\n",
      "\n",
      "[Epoch 4 | 8992/9024] loss: 8039.651855475\n",
      "  Training  | Loss:32146.6669 \n",
      "\n",
      "\n",
      "[Epoch 5 | 8992/9024] loss: 7465.160156258\n",
      "  Training  | Loss:31518.5653 \n",
      "\n",
      "\n",
      "[Epoch 6 | 8992/9024] loss: 8136.899414062\n",
      "  Training  | Loss:31155.4770 \n",
      "\n",
      "\n",
      "[Epoch 7 | 8992/9024] loss: 6589.414550788\n",
      "  Training  | Loss:30867.3413 \n",
      "\n",
      "\n",
      "[Epoch 8 | 8992/9024] loss: 8193.981445312\n",
      "  Training  | Loss:30724.5898 \n",
      "\n",
      "\n",
      "[Epoch 9 | 8992/9024] loss: 9662.478515620\n",
      "  Training  | Loss:30513.6936 \n",
      "\n",
      "\n",
      "[Epoch 10 | 8992/9024] loss: 6697.828125002\n",
      "  Training  | Loss:30347.2284 \n",
      "\n",
      "\n",
      "[Epoch 11 | 8992/9024] loss: 6380.072753912\n",
      "  Training  | Loss:30232.4404 \n",
      "\n",
      "\n",
      "[Epoch 12 | 8992/9024] loss: 7977.064941418\n",
      "  Training  | Loss:30120.1246 \n",
      "\n",
      "\n",
      "[Epoch 13 | 8992/9024] loss: 7497.204101568\n",
      "  Training  | Loss:29947.9200 \n",
      "\n",
      "\n",
      "[Epoch 14 | 8992/9024] loss: 7946.160156252\n",
      "  Training  | Loss:29944.2484 \n",
      "\n",
      "\n",
      "[Epoch 15 | 8992/9024] loss: 8746.482421880\n",
      "  Training  | Loss:29800.1536 \n",
      "\n",
      "\n",
      "[Epoch 16 | 8992/9024] loss: 6746.250000008\n",
      "  Training  | Loss:29759.8432 \n",
      "\n",
      "\n",
      "[Epoch 17 | 8992/9024] loss: 10690.02929688\n",
      "  Training  | Loss:29648.6864 \n",
      "\n",
      "\n",
      "[Epoch 18 | 8992/9024] loss: 6479.740722660\n",
      "  Training  | Loss:29569.3680 \n",
      "\n",
      "\n",
      "[Epoch 19 | 8992/9024] loss: 8811.780273448\n",
      "  Training  | Loss:29514.5262 \n",
      "\n",
      "\n",
      "[Epoch 20 | 8992/9024] loss: 6009.592285160\n",
      "  Training  | Loss:29459.4537 \n",
      "\n",
      "\n",
      "[Epoch 21 | 8992/9024] loss: 7598.939453122\n",
      "  Training  | Loss:29493.1844 \n",
      "\n",
      "\n",
      "[Epoch 22 | 8992/9024] loss: 9455.800781250\n",
      "  Training  | Loss:29415.6338 \n",
      "\n",
      "\n",
      "[Epoch 23 | 8992/9024] loss: 8422.019531250\n",
      "  Training  | Loss:29339.7176 \n",
      "\n",
      "\n",
      "[Epoch 24 | 8992/9024] loss: 7936.549316415\n",
      "  Training  | Loss:29318.9940 \n",
      "\n",
      "\n",
      "[Epoch 25 | 8992/9024] loss: 7588.105957035\n",
      "  Training  | Loss:29273.3009 \n",
      "\n",
      "\n",
      "[Epoch 26 | 8992/9024] loss: 6877.413085948\n",
      "  Training  | Loss:29216.4967 \n",
      "\n",
      "\n",
      "[Epoch 27 | 8992/9024] loss: 5695.007324228\n",
      "  Training  | Loss:29191.5010 \n",
      "\n",
      "\n",
      "[Epoch 28 | 8992/9024] loss: 7508.232421880\n",
      "  Training  | Loss:29181.9343 \n",
      "\n",
      "\n",
      "[Epoch 29 | 8992/9024] loss: 7107.907714842\n",
      "  Training  | Loss:29131.2299 \n",
      "\n",
      "\n",
      "[Epoch 30 | 8992/9024] loss: 7906.319824228\n",
      "  Training  | Loss:29071.0006 \n",
      "\n",
      "\n",
      "[Epoch 31 | 8992/9024] loss: 7701.526367198\n",
      "  Training  | Loss:29040.2060 \n",
      "\n",
      "\n",
      "[Epoch 32 | 8992/9024] loss: 7018.777343750\n",
      "  Training  | Loss:29070.6712 \n",
      "\n",
      "\n",
      "[Epoch 33 | 8992/9024] loss: 7538.884277348\n",
      "  Training  | Loss:29017.0057 \n",
      "\n",
      "\n",
      "[Epoch 34 | 8992/9024] loss: 7351.953613282\n",
      "  Training  | Loss:29001.2928 \n",
      "\n",
      "\n",
      "[Epoch 35 | 8992/9024] loss: 8199.362304690\n",
      "  Training  | Loss:28940.9737 \n",
      "\n",
      "\n",
      "[Epoch 36 | 8992/9024] loss: 9030.520507810\n",
      "  Training  | Loss:28916.5191 \n",
      "\n",
      "\n",
      "[Epoch 37 | 8992/9024] loss: 6431.263183590\n",
      "  Training  | Loss:28933.4329 \n",
      "\n",
      "\n",
      "[Epoch 38 | 8992/9024] loss: 6594.217285162\n",
      "  Training  | Loss:28893.0328 \n",
      "\n",
      "\n",
      "[Epoch 39 | 8992/9024] loss: 8007.205078128\n",
      "  Training  | Loss:28862.4220 \n",
      "\n",
      "\n",
      "[Epoch 40 | 8992/9024] loss: 7408.746582030\n",
      "  Training  | Loss:28854.9554 \n",
      "\n",
      "\n",
      "[Epoch 41 | 8992/9024] loss: 7349.630859382\n",
      "  Training  | Loss:28837.3697 \n",
      "\n",
      "\n",
      "[Epoch 42 | 8992/9024] loss: 6596.929687500\n",
      "  Training  | Loss:28807.5996 \n",
      "\n",
      "\n",
      "[Epoch 43 | 8992/9024] loss: 6905.940917972\n",
      "  Training  | Loss:28750.4485 \n",
      "\n",
      "\n",
      "[Epoch 44 | 8992/9024] loss: 7491.346679695\n",
      "  Training  | Loss:28768.7147 \n",
      "\n",
      "\n",
      "[Epoch 45 | 8992/9024] loss: 7678.326171880\n",
      "  Training  | Loss:28770.2415 \n",
      "\n",
      "\n",
      "[Epoch 46 | 8992/9024] loss: 5775.958984388\n",
      "  Training  | Loss:28757.8534 \n",
      "\n",
      "\n",
      "[Epoch 47 | 8992/9024] loss: 7272.761230470\n",
      "  Training  | Loss:28729.3881 \n",
      "\n",
      "\n",
      "[Epoch 48 | 8992/9024] loss: 9444.775390625\n",
      "  Training  | Loss:28728.9113 \n",
      "\n",
      "\n",
      "[Epoch 49 | 8992/9024] loss: 6680.090820310\n",
      "  Training  | Loss:28726.4186 \n",
      "\n",
      "\n",
      "[Epoch 50 | 8992/9024] loss: 8010.199707032\n",
      "  Training  | Loss:28683.0222 \n",
      "\n",
      "\n",
      "[Epoch 51 | 8992/9024] loss: 7794.551757812\n",
      "  Training  | Loss:28701.4380 \n",
      "\n",
      "\n",
      "[Epoch 52 | 8992/9024] loss: 6635.954589845\n",
      "  Training  | Loss:28692.4807 \n",
      "\n",
      "\n",
      "[Epoch 53 | 8992/9024] loss: 7073.586914065\n",
      "  Training  | Loss:28600.4600 \n",
      "\n",
      "\n",
      "[Epoch 54 | 8992/9024] loss: 7025.627441412\n",
      "  Training  | Loss:28625.7396 \n",
      "\n",
      "\n",
      "[Epoch 55 | 8992/9024] loss: 8132.870605475\n",
      "  Training  | Loss:28642.8416 \n",
      "\n",
      "\n",
      "[Epoch 56 | 8992/9024] loss: 7378.599609385\n",
      "  Training  | Loss:28587.5367 \n",
      "\n",
      "\n",
      "[Epoch 57 | 8992/9024] loss: 9641.778320310\n",
      "  Training  | Loss:28604.4681 \n",
      "\n",
      "\n",
      "[Epoch 58 | 8992/9024] loss: 6798.651855475\n",
      "  Training  | Loss:28621.5158 \n",
      "\n",
      "\n",
      "[Epoch 59 | 8992/9024] loss: 6799.484863280\n",
      "  Training  | Loss:28569.7628 \n",
      "\n",
      "\n",
      "[Epoch 60 | 8992/9024] loss: 6072.846191415\n",
      "  Training  | Loss:28572.5414 \n",
      "\n",
      "\n",
      "[Epoch 61 | 8992/9024] loss: 6607.722167970\n",
      "  Training  | Loss:28586.5447 \n",
      "\n",
      "\n",
      "[Epoch 62 | 8992/9024] loss: 6130.497558595\n",
      "  Training  | Loss:28579.4264 \n",
      "\n",
      "\n",
      "[Epoch 63 | 8992/9024] loss: 7483.971191418\n",
      "  Training  | Loss:28553.9537 \n",
      "\n",
      "\n",
      "[Epoch 64 | 8992/9024] loss: 6779.530761720\n",
      "  Training  | Loss:28540.3336 \n",
      "\n",
      "\n",
      "[Epoch 65 | 8992/9024] loss: 7919.734863282\n",
      "  Training  | Loss:28537.1305 \n",
      "\n",
      "\n",
      "[Epoch 66 | 8992/9024] loss: 7983.365234385\n",
      "  Training  | Loss:28536.1413 \n",
      "\n",
      "\n",
      "[Epoch 67 | 8992/9024] loss: 8832.015625005\n",
      "  Training  | Loss:28515.8877 \n",
      "\n",
      "\n",
      "[Epoch 68 | 8992/9024] loss: 6461.653808598\n",
      "  Training  | Loss:28508.2976 \n",
      "\n",
      "\n",
      "[Epoch 69 | 8992/9024] loss: 7431.859375000\n",
      "  Training  | Loss:28510.7580 \n",
      "\n",
      "\n",
      "[Epoch 70 | 8992/9024] loss: 8644.260742190\n",
      "  Training  | Loss:28460.2656 \n",
      "\n",
      "\n",
      "[Epoch 71 | 8992/9024] loss: 5615.874023440\n",
      "  Training  | Loss:28490.9914 \n",
      "\n",
      "\n",
      "[Epoch 72 | 8992/9024] loss: 7438.715332038\n",
      "  Training  | Loss:28476.6937 \n",
      "\n",
      "\n",
      "[Epoch 73 | 8992/9024] loss: 6719.296386728\n",
      "  Training  | Loss:28463.1257 \n",
      "\n",
      "\n",
      "[Epoch 74 | 8992/9024] loss: 8200.749023442\n",
      "  Training  | Loss:28448.7594 \n",
      "\n",
      "\n",
      "[Epoch 75 | 8992/9024] loss: 5954.060546888\n",
      "  Training  | Loss:28447.0132 \n",
      "\n",
      "\n",
      "[Epoch 76 | 8992/9024] loss: 8238.708984382\n",
      "  Training  | Loss:28512.2657 \n",
      "\n",
      "\n",
      "[Epoch 77 | 8992/9024] loss: 8900.984375002\n",
      "  Training  | Loss:28469.7219 \n",
      "\n",
      "\n",
      "[Epoch 78 | 8992/9024] loss: 7887.674804692\n",
      "  Training  | Loss:28451.6505 \n",
      "\n",
      "\n",
      "[Epoch 79 | 8992/9024] loss: 5069.979980475\n",
      "  Training  | Loss:28397.1879 \n",
      "\n",
      "\n",
      "[Epoch 80 | 8992/9024] loss: 8424.314453128\n",
      "  Training  | Loss:28391.2180 \n",
      "\n",
      "\n",
      "[Epoch 81 | 8992/9024] loss: 7465.778320312\n",
      "  Training  | Loss:28377.0225 \n",
      "\n",
      "\n",
      "[Epoch 82 | 8992/9024] loss: 6982.594726568\n",
      "  Training  | Loss:28400.1846 \n",
      "\n",
      "\n",
      "[Epoch 83 | 8992/9024] loss: 7602.644531250\n",
      "  Training  | Loss:28425.2897 \n",
      "\n",
      "\n",
      "[Epoch 84 | 8992/9024] loss: 8587.737304695\n",
      "  Training  | Loss:28387.1519 \n",
      "\n",
      "\n",
      "[Epoch 85 | 8992/9024] loss: 5465.215332030\n",
      "  Training  | Loss:28415.9790 \n",
      "\n",
      "\n",
      "[Epoch 86 | 8992/9024] loss: 8760.009765622\n",
      "  Training  | Loss:28392.5352 \n",
      "\n",
      "\n",
      "[Epoch 87 | 8992/9024] loss: 5751.187011728\n",
      "  Training  | Loss:28362.3696 \n",
      "\n",
      "\n",
      "[Epoch 88 | 8992/9024] loss: 9105.330078125\n",
      "  Training  | Loss:28365.1246 \n",
      "\n",
      "\n",
      "[Epoch 89 | 8992/9024] loss: 5876.889160160\n",
      "  Training  | Loss:28350.3829 \n",
      "\n",
      "\n",
      "[Epoch 90 | 8992/9024] loss: 6854.702148445\n",
      "  Training  | Loss:28351.4960 \n",
      "\n",
      "\n",
      "[Epoch 91 | 8992/9024] loss: 6515.944335945\n",
      "  Training  | Loss:28392.5287 \n",
      "\n",
      "\n",
      "[Epoch 92 | 8992/9024] loss: 6548.745605475\n",
      "  Training  | Loss:28394.2377 \n",
      "\n",
      "\n",
      "[Epoch 93 | 8992/9024] loss: 7410.879394532\n",
      "  Training  | Loss:28386.2419 \n",
      "\n",
      "\n",
      "[Epoch 94 | 8992/9024] loss: 5626.296386722\n",
      "  Training  | Loss:28341.6147 \n",
      "\n",
      "\n",
      "[Epoch 95 | 8992/9024] loss: 8047.295410160\n",
      "  Training  | Loss:28346.3457 \n",
      "\n",
      "\n",
      "[Epoch 96 | 8992/9024] loss: 6392.429687500\n",
      "  Training  | Loss:28332.7549 \n",
      "\n",
      "\n",
      "[Epoch 97 | 8992/9024] loss: 6966.929199220\n",
      "  Training  | Loss:28320.9715 \n",
      "\n",
      "\n",
      "[Epoch 98 | 8992/9024] loss: 8764.489257815\n",
      "  Training  | Loss:28292.9786 \n",
      "\n",
      "\n",
      "[Epoch 99 | 8992/9024] loss: 6320.954589848\n",
      "  Training  | Loss:28336.3390 \n",
      "\n",
      "\n",
      "[Epoch 100 | 8992/9024] loss: 6854.070800782\n",
      "  Training  | Loss:28310.0838 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We set criterion : L1 loss (or Mean Absolute Error, MAE)\n",
    "\n",
    "optimizerG = torch.optim.Adam(Gnet.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "optimizerD = torch.optim.Adam(Dnet.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Now, we train 20 epochs.\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss, best_loss = 0, 100\n",
    "    \"\"\"csie ta code\n",
    "    for x in train_dataloader:\n",
    "\n",
    "        latent, reconstruct = model(x)\n",
    "        loss = criterion(reconstruct, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cumulate_loss = loss.item() * x.shape[0]\n",
    "\n",
    "    print(f'Epoch { \"%03d\" % (epoch+1) }: Loss : { \"%.8f\" % (cumulate_loss / trainX.shape[0])}')\n",
    "    \"\"\"\n",
    "\n",
    "    for idx, image in enumerate(train_dataloader):\n",
    "        reconsturct , mean, var = model(image)\n",
    "        loss, bce, kld = loss_function(reconsturct, image, mean, var)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += (loss.item() / len(train_dataloader))\n",
    "        print('[Epoch %d | %d/%d] loss: %.8f' %((epoch+1), idx*32, len(train_dataloader)*32, loss.item()), end='\\r')\n",
    "    print(\"\\n  Training  | Loss:%.4f \\n\\n\" % total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_space finish\n",
      "(9000, 128)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Collect the latents and stdardize it.\n",
    "latents = []\n",
    "latent_sapce = []\n",
    "for x in test_dataloader:\n",
    "    _,mu,var = model(x)\n",
    "    mu = mu.detach().cpu().numpy()\n",
    "    for i in range(mu.shape[0]):\n",
    "        latent_sapce.append(mu[i])\n",
    "        \n",
    "print('latent_space finish')\n",
    "latent_space = np.asarray(latent_sapce)\n",
    "\n",
    "print(latent_space.shape)\n",
    "latents = (latent_space - np.mean(latent_space, axis=0)) / np.std(latent_space, axis=0)\n",
    "\n",
    "# Use PCA to lower dim of latents and use K-means to clustering.\n",
    "pca = PCA(n_components=32, copy=False, whiten=True, svd_solver='full')\n",
    "latent_vec = pca.fit_transform(latents)\n",
    "latent_vec = TSNE(n_components = 3).fit_transform(latent_vec)\n",
    "result = KMeans(n_clusters=2, random_state=2, max_iter=1000).fit(latent_vec).labels_\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "latents = PCA(n_components=16).fit_transform(latents)\n",
    "result = KMeans(n_clusters = 2).fit(latents).labels_\n",
    "\"\"\"\n",
    "# We know first 5 labels are zeros, it's a mechanism to check are your answers\n",
    "# need to be flipped or not.\n",
    "print(np.sum(result[:5]))\n",
    "if np.sum(result[:5]) >= 3:\n",
    "    result = 1 - result\n",
    "\"\"\"\"\n",
    "if np.sum(result[:5]) != 0 or np.sum(result[:5])!=5:\n",
    "    print(\"redo\")\n",
    "\"\"\"\n",
    "# Generate your submission\n",
    "df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
    "df.to_csv('baseline_11_14_23_30.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
