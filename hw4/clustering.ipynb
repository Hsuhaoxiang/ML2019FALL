{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2\n",
    "import argparse\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "# other library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "# PyTorch library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # define: encoder\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, 3, 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 8, 3, 1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(968, 512)\n",
    "        self.fc2 = nn.Linear(512, 968)\n",
    "        # define: decoder\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 4, 2, dilation=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16,8, 5, 1, dilation=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 3, 5, 1, dilation=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"x_shape:\",x.shape)\n",
    "        \n",
    "        #########################Encode######################################## \n",
    "        featuremap1 = self.conv1(x)\n",
    "        #print(\"feature map shape:\",featuremap1.shape)\n",
    "        \n",
    "        \n",
    "        featuremap2 = self.conv2(featuremap1)\n",
    "        #print(\"feature map2 shape:\",featuremap2.shape)\n",
    "        \n",
    "        \n",
    "        featuremap3 = self.conv3(featuremap2)\n",
    "        #print(\"feature map3 shape:\",featuremap3.shape)\n",
    "\n",
    "        \n",
    "        code = featuremap3.view(len(featuremap3), -1)\n",
    "        latent1 = self.fc1(code)\n",
    "        latent2 = F.relu(self.fc2(latent1))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #########################Decode######################################## \n",
    "        x = latent2.view(-1, 8, 11, 11)\n",
    "        #print(\"x shape:\",x.shape)\n",
    "        \n",
    "        rec1 = self.decoder1(x)\n",
    "        #print(\"rec1 shape:\",rec1.shape)\n",
    "        \n",
    "        rec2 = self.decoder2(rec1)\n",
    "        #print(\"rec2 shape:\",rec2.shape)\n",
    "        \n",
    "        decoded = self.decoder3(rec2)\n",
    "        #print(\"decoded shape:\",decoded.shape)\n",
    "        \n",
    "        \n",
    "        # Total AE: return latent & reconstruct\n",
    "        return latent1, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# detect is gpu available.\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    device =torch.device('cuda:0')\n",
    "else:\n",
    "    device =torch.device(\"cpu\")\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "# load data and normalize to [-1, 1]\n",
    "trainX = np.load('./trainX.npy')\n",
    "print(trainX.shape)\n",
    "trainX = np.transpose(trainX, (0, 3, 1, 2))/ 255.*2 -1\n",
    "trainX = torch.Tensor(trainX)\n",
    "\n",
    "\n",
    "# if use_gpu, send model / data to GPU.\n",
    "if use_gpu:\n",
    "    autoencoder.cuda()\n",
    "    trainX = trainX.cuda()\n",
    "\n",
    "# Dataloader: train shuffle = True\n",
    "train_dataloader = DataLoader(trainX, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(trainX, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | 8992/9024] loss: 0.1036\n",
      "  Training  | Loss:0.1381 \n",
      "[Epoch 2 | 8992/9024] loss: 0.1390\n",
      "  Training  | Loss:0.0865 \n",
      "[Epoch 3 | 2752/9024] loss: 0.0831\r"
     ]
    }
   ],
   "source": [
    "# We set criterion : L1 loss (or Mean Absolute Error, MAE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Now, we train 20 epochs.\n",
    "for epoch in range(80):\n",
    "    autoencoder.train()\n",
    "    total_loss, best_loss = 0, 100\n",
    "    \"\"\"csie ta code\n",
    "    for x in train_dataloader:\n",
    "\n",
    "        latent, reconstruct = autoencoder(x)\n",
    "        loss = criterion(reconstruct, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cumulate_loss = loss.item() * x.shape[0]\n",
    "\n",
    "    print(f'Epoch { \"%03d\" % (epoch+1) }: Loss : { \"%.8f\" % (cumulate_loss / trainX.shape[0])}')\n",
    "    \"\"\"\n",
    "\n",
    "    for idx, image in enumerate(train_dataloader):\n",
    "        image = image.to(device, dtype=torch.float)\n",
    "        _, reconsturct = autoencoder(image)\n",
    "        loss = criterion(reconsturct, image)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += (loss.item() / len(train_dataloader))\n",
    "\n",
    "        print('[Epoch %d | %d/%d] loss: %.4f' %((epoch+1), idx*32, len(train_dataloader)*32, loss.item()), end='\\r')\n",
    "    print(\"\\n  Training  | Loss:%.4f \" % total_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the latents and stdardize it.\n",
    "latents = []\n",
    "reconstructs = []\n",
    "for x in test_dataloader:\n",
    "    latent, reconstruct = autoencoder(x)\n",
    "    latents.append(latent.cpu().detach().numpy())\n",
    "    reconstructs.append(reconstruct.cpu().detach().numpy())\n",
    "\n",
    "latents = np.concatenate(latents, axis=0).reshape([9000, -1])\n",
    "latents = (latents - np.mean(latents, axis=0)) / np.std(latents, axis=0)\n",
    "print(latents.shape)\n",
    "\n",
    "# Use PCA to lower dim of latents and use K-means to clustering.\n",
    "pca = PCA(n_components=100, copy=False, whiten=True, svd_solver='full')\n",
    "latent_vec = pca.fit_transform(latents)\n",
    "result = KMeans(n_clusters=2, random_state=2, max_iter=1000).fit(latent_vec)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "latents = PCA(n_components=16).fit_transform(latents)\n",
    "result = KMeans(n_clusters = 2).fit(latents).labels_\n",
    "\"\"\"\n",
    "# We know first 5 labels are zeros, it's a mechanism to check are your answers\n",
    "# need to be flipped or not.\n",
    "print(np.sum(result[:5]))\n",
    "if np.sum(result[:5]) >= 3:\n",
    "    result = 1 - result\n",
    "\"\"\"\"\n",
    "if np.sum(result[:5]) != 0 or np.sum(result[:5])!=5:\n",
    "    print(\"redo\")\n",
    "\"\"\"\n",
    "# Generate your submission\n",
    "df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
    "df.to_csv('baseline.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
