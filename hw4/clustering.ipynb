{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # define: encoder\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 8, 4, 2,padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # define: decoder\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, 2,dilation=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 8, 3, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 3, 2, 1,dilation=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"x_shape:\",x.shape)\n",
    "        featuremap1 = self.conv1(x)\n",
    "        #print(\"feature map shape:\",featuremap1.shape)\n",
    "        featuremap2 = self.conv2(featuremap1)\n",
    "        #print(\"feature map2 shape:\",featuremap2.shape)\n",
    "        #featuremap3 = self.conv3(featuremap2)\n",
    "        #print(\"feature map3 shape:\",featuremap3.shape)        \n",
    "        rec1 = self.decoder1(featuremap2)\n",
    "        #print(\"rec1 shape:\",rec1.shape)\n",
    "        rec2 = self.decoder2(rec1)\n",
    "        #print(\"rec2 shape:\",rec2.shape)\n",
    "        decoded = self.decoder3(rec2)\n",
    "        #print(\"decoded shape:\",decoded.shape)\n",
    "        # Total AE: return latent & reconstruct\n",
    "        return featuremap2, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# detect is gpu available.\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "# load data and normalize to [-1, 1]\n",
    "trainX = np.load('./trainX.npy')\n",
    "print(trainX.shape)\n",
    "trainX = np.transpose(trainX, (0, 3, 1, 2)) / 255. * 2 - 1\n",
    "trainX = torch.Tensor(trainX)\n",
    "\n",
    "# if use_gpu, send model / data to GPU.\n",
    "if use_gpu:\n",
    "    autoencoder.cuda()\n",
    "    trainX = trainX.cuda()\n",
    "\n",
    "# Dataloader: train shuffle = True\n",
    "train_dataloader = DataLoader(trainX, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(trainX, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss : 0.00004138\n",
      "Epoch 002: Loss : 0.00003589\n",
      "Epoch 003: Loss : 0.00003140\n",
      "Epoch 004: Loss : 0.00001984\n",
      "Epoch 005: Loss : 0.00003483\n",
      "Epoch 006: Loss : 0.00002756\n",
      "Epoch 007: Loss : 0.00002267\n",
      "Epoch 008: Loss : 0.00002349\n",
      "Epoch 009: Loss : 0.00003651\n",
      "Epoch 010: Loss : 0.00003458\n",
      "Epoch 011: Loss : 0.00001775\n",
      "Epoch 012: Loss : 0.00002427\n",
      "Epoch 013: Loss : 0.00001786\n",
      "Epoch 014: Loss : 0.00003790\n",
      "Epoch 015: Loss : 0.00002921\n",
      "Epoch 016: Loss : 0.00001875\n",
      "Epoch 017: Loss : 0.00003671\n",
      "Epoch 018: Loss : 0.00002097\n",
      "Epoch 019: Loss : 0.00003646\n",
      "Epoch 020: Loss : 0.00002180\n"
     ]
    }
   ],
   "source": [
    "# We set criterion : L1 loss (or Mean Absolute Error, MAE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Now, we train 20 epochs.\n",
    "for epoch in range(20):\n",
    "\n",
    "    cumulate_loss = 0\n",
    "    for x in train_dataloader:\n",
    "\n",
    "        latent, reconstruct = autoencoder(x)\n",
    "        loss = criterion(reconstruct, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cumulate_loss = loss.item() * x.shape[0]\n",
    "\n",
    "    print(f'Epoch { \"%03d\" % (epoch+1) }: Loss : { \"%.8f\" % (cumulate_loss / trainX.shape[0])}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 392)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Collect the latents and stdardize it.\n",
    "latents = []\n",
    "reconstructs = []\n",
    "for x in test_dataloader:\n",
    "    latent, reconstruct = autoencoder(x)\n",
    "    latents.append(latent.cpu().detach().numpy())\n",
    "    reconstructs.append(reconstruct.cpu().detach().numpy())\n",
    "\n",
    "latents = np.concatenate(latents, axis=0).reshape([9000, -1])\n",
    "latents = (latents - np.mean(latents, axis=0)) / np.std(latents, axis=0)\n",
    "print(latents.shape)\n",
    "\n",
    "# Use PCA to lower dim of latents and use K-means to clustering.\n",
    "latents = PCA(n_components=16).fit_transform(latents)\n",
    "result = KMeans(n_clusters = 2).fit(latents).labels_\n",
    "\n",
    "# We know first 5 labels are zeros, it's a mechanism to check are your answers\n",
    "# need to be flipped or not.\n",
    "print(np.sum(result[:5]))\n",
    "if np.sum(result[:5]) >= 3:\n",
    "    result = 1 - result\n",
    "\"\"\"\"\n",
    "if np.sum(result[:5]) != 0 or np.sum(result[:5])!=5:\n",
    "    print(\"redo\")\n",
    "\"\"\"\n",
    "# Generate your submission\n",
    "df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
    "df.to_csv('baseline.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
