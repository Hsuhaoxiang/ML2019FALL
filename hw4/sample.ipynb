{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from skimage.io import imread, imsave\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import os\n",
    "import torchvision\n",
    "import argparse\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        self.conv_3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        self.conv_4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.deconv_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        self.deconv_2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "        self.deconv_3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, padding=1,bias=False)\n",
    "        )\n",
    "        # final output activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \"\"\"\"\n",
    "    def encode(self, x):\n",
    "        conv_output = self.conv_stage(x).view(-1, 1024)\n",
    "        return self.fcMean(conv_output), self.fcStd(conv_output)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        #eps = Variable(eps).cuda()\n",
    "        #eps = eps.cuda()\n",
    "        eps.requires_grad=True\n",
    "        eps = eps.cuda()\n",
    "        #print(eps.requires_grad)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "\n",
    "    def decode(self, z):\n",
    "        fc_output = self.fcDecode(z).view(-1, 256, 2, 2)\n",
    "        trans_conv_output = self.trans_conv_stage(fc_output)\n",
    "        return self.tanh(trans_conv_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        feature1 = self.conv_1(x)\n",
    "        #print(\"latent shape:\",latent.shape)\n",
    "        feature2 = self.conv_2(feature1)\n",
    "        #print(\"latent shape:\",latent.shape)\n",
    "        feature3 = self.conv_3(feature2)\n",
    "        #print(\"latent shape:\",latent.shape)\n",
    "        latent = self.conv_4(feature3)\n",
    "        #print(\"latent shape:\",latent.shape)\n",
    "        reconstruct = self.trans_conv_stage(latent)\n",
    "        #print(\"reconstruct shape\",reconstruct.shape)\n",
    "        return latent , self.tanh(reconstruct)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect is gpu available.\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "autoencoder = VAE()\n",
    "\n",
    "# load data and normalize to [-1, 1]\n",
    "trainX = np.load('./trainX.npy')\n",
    "trainX = np.transpose(trainX, (0, 3, 1, 2))\n",
    "trainX = torch.Tensor(trainX)\n",
    "\n",
    "# if use_gpu, send model / data to GPU.\n",
    "if use_gpu:\n",
    "    autoencoder.cuda()\n",
    "    trainX = trainX.cuda()\n",
    "\n",
    "# Dataloader: train shuffle = True\n",
    "train_dataloader = DataLoader(trainX, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(trainX, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss : 2.01379319\n",
      "Epoch 002: Loss : 2.20060135\n",
      "Epoch 003: Loss : 2.43790560\n",
      "Epoch 004: Loss : 2.12923785\n",
      "Epoch 005: Loss : 2.31689084\n",
      "Epoch 006: Loss : 2.03379232\n",
      "Epoch 007: Loss : 2.32218837\n",
      "Epoch 008: Loss : 1.88432791\n",
      "Epoch 009: Loss : 2.02041710\n",
      "Epoch 010: Loss : 2.01255252\n",
      "Epoch 011: Loss : 2.16809223\n",
      "Epoch 012: Loss : 1.90897852\n",
      "Epoch 013: Loss : 1.65889963\n",
      "Epoch 014: Loss : 2.04051020\n",
      "Epoch 015: Loss : 2.25630490\n",
      "Epoch 016: Loss : 2.30282205\n",
      "Epoch 017: Loss : 1.95870638\n",
      "Epoch 018: Loss : 2.46334505\n",
      "Epoch 019: Loss : 2.01727322\n",
      "Epoch 020: Loss : 1.61044086\n",
      "Epoch 021: Loss : 1.96810981\n",
      "Epoch 022: Loss : 1.88112109\n",
      "Epoch 023: Loss : 1.95636306\n",
      "Epoch 024: Loss : 2.29225651\n",
      "Epoch 025: Loss : 1.70298763\n",
      "Epoch 026: Loss : 3.38758941\n",
      "Epoch 027: Loss : 1.91188477\n",
      "Epoch 028: Loss : 1.87479145\n",
      "Epoch 029: Loss : 2.29586328\n",
      "Epoch 030: Loss : 1.87884787\n",
      "Epoch 031: Loss : 2.99515929\n",
      "Epoch 032: Loss : 1.94929557\n",
      "Epoch 033: Loss : 2.15267773\n",
      "Epoch 034: Loss : 2.03920942\n",
      "Epoch 035: Loss : 1.91019336\n",
      "Epoch 036: Loss : 2.38066146\n",
      "Epoch 037: Loss : 1.76563129\n",
      "Epoch 038: Loss : 2.31381380\n",
      "Epoch 039: Loss : 2.36725673\n",
      "Epoch 040: Loss : 1.40843012\n",
      "Epoch 041: Loss : 1.95104644\n",
      "Epoch 042: Loss : 2.07339996\n",
      "Epoch 043: Loss : 2.01863411\n",
      "Epoch 044: Loss : 1.76340603\n",
      "Epoch 045: Loss : 1.58952865\n",
      "Epoch 046: Loss : 1.65700282\n",
      "Epoch 047: Loss : 1.79782465\n",
      "Epoch 048: Loss : 1.53373275\n",
      "Epoch 049: Loss : 1.79737109\n",
      "Epoch 050: Loss : 2.03805208\n",
      "Epoch 051: Loss : 2.25710872\n",
      "Epoch 052: Loss : 1.96455773\n",
      "Epoch 053: Loss : 2.20239670\n",
      "Epoch 054: Loss : 3.02518186\n",
      "Epoch 055: Loss : 2.09118207\n",
      "Epoch 056: Loss : 1.91874631\n",
      "Epoch 057: Loss : 1.84043316\n",
      "Epoch 058: Loss : 1.85076172\n",
      "Epoch 059: Loss : 1.80295812\n",
      "Epoch 060: Loss : 1.93494293\n"
     ]
    }
   ],
   "source": [
    "# We set criterion : L1 loss (or Mean Absolute Error, MAE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Now, we train 20 epochs.\n",
    "for epoch in range(100):\n",
    "\n",
    "    cumulate_loss = 0\n",
    "    for x in train_dataloader:\n",
    "        \n",
    "        latent, reconstruct = autoencoder(x)\n",
    "        loss = criterion(reconstruct, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cumulate_loss = loss.item() * x.shape[0]\n",
    "\n",
    "    print(f'Epoch { \"%03d\" % (epoch+1) }: Loss : { \"%.8f\" % (cumulate_loss / trainX.shape[0])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the latents and stdardize it.\n",
    "latents = []\n",
    "reconstructs = []\n",
    "for x in test_dataloader:\n",
    "    latent, reconstruct = autoencoder(x)\n",
    "    latents.append(latent.cpu().detach().numpy())\n",
    "    reconstructs.append(reconstruct.cpu().detach().numpy())\n",
    "\n",
    "latents = np.concatenate(latents, axis=0).reshape([9000, -1])\n",
    "latents = (latents - np.mean(latents, axis=0)) / np.std(latents, axis=0)\n",
    "print(latents.shape)\n",
    "\n",
    "# Use PCA to lower dim of latents and use K-means to clustering.\n",
    "latents = PCA(n_components=8).fit_transform(latents)\n",
    "result = KMeans(n_clusters = 2).fit(latents).labels_\n",
    "\n",
    "# We know first 5 labels are zeros, it's a mechanism to check are your answers\n",
    "# need to be flipped or not.\n",
    "print(np.sum(result[:5]))\n",
    "if np.sum(result[:5]) >= 3:\n",
    "    result = 1 - result\n",
    "\"\"\"\"\n",
    "if np.sum(result[:5]) != 0 or np.sum(result[:5])!=5:\n",
    "    print(\"redo\")\n",
    "\"\"\"\n",
    "# Generate your submission\n",
    "df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
    "df.to_csv('baseline.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
